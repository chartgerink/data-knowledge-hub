{"searchDocs":[{"title":"How to contribute?","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/contribute/06_01_how-to-contribute","content":"","keywords":"","version":"Next"},{"title":"Options for contributing to the data knowledge hub​","type":1,"pageTitle":"How to contribute?","url":"/data-knowledge-hub/docs/contribute/06_01_how-to-contribute#options-for-contributing-to-the-data-knowledge-hub","content":" The Data Knowledge Hub is a collaborative open-source project. We always welcome contributions from the community – either via GitHub or by contacting us directly. To make participation in our community an open, welcoming, diverse, inclusive, and healthy experience for everyone, the Data Knowledge Hub is released with a Code of Conduct. By contributing to this project, you agree to abide by its terms.  ","version":"Next","tagName":"h2"},{"title":"Option 1: Pull the project and create a merge request on GitHub​","type":1,"pageTitle":"How to contribute?","url":"/data-knowledge-hub/docs/contribute/06_01_how-to-contribute#option-1-pull-the-project-and-create-a-merge-request-on-github","content":" Here's how you can contribute directly over GitHub:  Open the Data Knowledge Hub on GitHubFork your own copy of the repository to your personal accountMake your changes and commit them.Open a pull request on GitHub and fill out all the necessary information.Wait for the maintainers to review your changes.Once your changes are approved, they will be merged into the main branch.  Please note: Especially minor edits, corrections or smaller additions can be submitted very conveniently via GitHub. For longer contributions or a series of changes we would recommend that you reach out to us via Option 2.  ","version":"Next","tagName":"h3"},{"title":"Option 2: Contact us and we will integrate your changes and content together​","type":1,"pageTitle":"How to contribute?","url":"/data-knowledge-hub/docs/contribute/06_01_how-to-contribute#option-2-contact-us-and-we-will-integrate-your-changes-and-content-together","content":" We welcome contributions on a rolling basis. Whether you have a practical example you want to share, or just an idea for any content that is missing: Get in touch with us at upgrade.democracy@bertelsmann-stiftung.de  ","version":"Next","tagName":"h3"},{"title":"What content are we looking for currently?​","type":1,"pageTitle":"How to contribute?","url":"/data-knowledge-hub/docs/contribute/06_01_how-to-contribute#what-content-are-we-looking-for-currently","content":" Contributions can take on a lot of different forms. In general, we are looking for:  Python or R Notebooks: Well-commented notebooks illustrating libraries or specific analysis processes.Practical Guidelines and Processes: Short blog posts (~1000-2000 words) with code examples, checklists and descriptions of processes. We are also interested in overviews or link collections, e.g. data access points or insightful research.Introductory Content and Context: Essays (~3000 words) providing context and practical information - such as legal and ethical guidelines, profiles for specific platforms, or research methodologies.  info Right now, we would be particularly interested in including and discussing chapters on: Data access and ethics: Data access rights beyond the European Union and the U.S.How to deal with dark socials? Data collection: sock puppet, snowball sampling and other innovative approachesExamples of data collection: Facebook, Instagram, YouTube, Fediverse and othersExamples of data analysis: Topic modelling, sentiment analysis, geospatial analysis, infrastructure as code, and othersAdditional aspects that benefit from monitoring as a research method ","version":"Next","tagName":"h2"},{"title":"Contributors","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/contribute/06_02_community","content":"","keywords":"","version":"Next"},{"title":"Facilitation and core team​","type":1,"pageTitle":"Contributors","url":"/data-knowledge-hub/docs/contribute/06_02_community#facilitation-and-core-team","content":" CB Cathleen Berger Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn   CF Charlotte Freihse Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn   CR Clara Ruthardt Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn   JM Johannes Müller &amp;effect data solutions GmbH Website Twitter / X Mastodon Google Scholar LinkedIn   ","version":"Next","tagName":"h2"},{"title":"Knowledge contributors​","type":1,"pageTitle":"Contributors","url":"/data-knowledge-hub/docs/contribute/06_02_community#knowledge-contributors","content":" AN Andreas Neumeier SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn   AP Alessandro Polidoro attorney at law &amp; digital rights advocate Website Twitter / X Mastodon Google Scholar LinkedIn   CA Christoph M. Abels University of Potsdam Website Twitter / X Mastodon Google Scholar LinkedIn   HT Heather Dannyelle Thompson Democracy Reporting International Website Twitter / X Mastodon Google Scholar LinkedIn   JR Jasmin Riedl SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn   JH Josef Holnburger Center for Monitoring, Analysis and Strategy (CeMAS) Website Twitter / X Mastodon Google Scholar LinkedIn   MD Martin Degeling Stiftung Neue Verantwortung Website Twitter / X Mastodon Google Scholar LinkedIn   OKFN Open Knowledge Foundation OKFN Website Twitter / X Mastodon Google Scholar LinkedIn   MD Philipp Darius Hertie School of Governance Website Twitter / X Mastodon Google Scholar LinkedIn   WD Wiebke Drews SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn  ","version":"Next","tagName":"h2"},{"title":"The Data Knowledge Hub for Researching Online Discourse","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/background-rationale","content":"","keywords":"","version":"Next"},{"title":"Background and rationale: What, why, and how does it help you?​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#background-and-rationale-what-why-and-how-does-it-help-you","content":"   CB Cathleen Berger Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn   CF Charlotte Freihse Upgrade Democracy | Bertelsmann Stiftung Website Twitter / X Mastodon Google Scholar LinkedIn   The Data Knowledge Hub for Researching Online Discourse (Data Knowledge Hub) is an initiative that aims to provide a central resource for researchers, social scientists, data scientists, journalists, and other practitioners, and policy makers interested in monitoring social media and online discourse more broadly.    ","version":"Next","tagName":"h2"},{"title":"Why do we feel this is necessary?​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#why-do-we-feel-this-is-necessary","content":" Online discourse has changed how we inform ourselves, what and who to trust, as well as how information is quite simply accessed. Notably on online platforms and social media, recommender systems and other design features can be gamed to fuel disinformation, hate speech, and outrage. In addition, messaging services and alternative platforms are increasingly falling risk to exploitation and provide agitators with vast audiences to spread falsehoods. But how and why exactly this is happening remains under-researched and merely anecdotally illustrated. If we want to strengthen our information ecosystem and increase each other’s ability to decide what’s trustworthy and what’s not, we need to move away from anecdotes towards broad, continuous, and ideally real-time data-driven insight.  ","version":"Next","tagName":"h3"},{"title":"The challenge​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#the-challenge","content":" Due to the increasing number of social media and other digital platforms as well as the huge amounts of data to analyse, it is critical to enable and empower more researchers, social as well as data scientists, on two fronts:  to conduct independent research of social media and online discourse on a technical level, andto assess the data from a socio-political context.  There are already renowned, well-established organisations that do incredible work on Social Media Monitoring, including CeMAS, Democracy Reporting International, the SPARTA Project of the Bundeswehr University Munich, or the Institute for Strategic Dialogue. Yet even these established players face several challenges, among others:  the multitude of digital platforms;the sheer amount of data and necessary server capacities;fast-developing and constantly changing narratives;new and changing actors and agitators.  ","version":"Next","tagName":"h3"},{"title":"Building a foundation for solving these challenges​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#building-a-foundation-for-solving-these-challenges","content":" To reduce the obstacles and lower the threshold to monitoring online discourse, we are launching this Data Knowledge Hub. Hosted open source and under a Creative Commons license on GitHub, it continuously welcomes contributions of new data, code, and written content, fostering a collaborative environment for all. Cooperation and collaboration on development, design, content, and scope among established actors is key to turning this Data Knowledge Hub into a useful tool and an enabler for future research.  For first publication in September 2023, we gathered initial contributions on legal basis and ethical standards, good practices and exemplary research for webscraping, data collection on Twitter and TikTok as well as code samples to monitor various platforms. This Data Knowledge Hub will be continuously updated and reviewed, and, with the help of community and crowdsourced contributions, we hope to include a broad range of samples and organic input, over time providing all relevant information for monitoring and understanding the dynamics of online discourse.  ","version":"Next","tagName":"h3"},{"title":"You can help and contribute, too​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#you-can-help-and-contribute-too","content":" We welcome additional contributions on a rolling basis. Right now, we would be particularly interested in including and discussing chapters on:  Social media usage: users worldwide, number of posts/messages, regional differences etc.Data access and ethics: How to deal with dark socials?Data access rights beyond the European Union and the U.S. Data collection: sock puppet, snowball sampling and other innovative approachesExamples of data collection: Facebook, Instagram, YouTube, Fediverse and othersExamples of data analysis: Topic modelling, sentiment analysis, geospational analysis, infrastructure as code, and othersAdditional aspects that benefit from monitoring as a research method  ","version":"Next","tagName":"h3"},{"title":"Living document - How to navigate the Data Knowledge Hub​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#living-document---how-to-navigate-the-data-knowledge-hub","content":" JM Johannes Müller &amp;effect data solutions GmbH Website Twitter / X Mastodon Google Scholar LinkedIn   The Data Knowledge Hub is hosted on a GitHub repository. For better usability we use a documentation framework which allows users to switch to a static website for easier reading, accessing content as a digital book. This means that all text content is created using Markdown. Code projects are included as a single file (e.g. a Jupyter Notebook) or in folders that can be pulled from GitHub. We intend to continuously update content and invite contributions on additional aspects of monitoring social media and online discourse. A first version was published in September 2023, chapters that are already in the pipeline are marked as “forthcoming”, a list of invited contributions can be found in the “editorial”.  All contributors are listed here as well as named in their respective chapters.  ","version":"Next","tagName":"h2"},{"title":"Code projects​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#code-projects","content":" Here is a table with all projects that are currently included in the Data Knowledge Hub. Click on the link to go to the project page.  Project\tDescription\tLanguage\tPlattform\tCodetiktok-scraping\tCollect data on TikTok using puppeteer JavaScript\tTikTok\tCode tiktok-hashtag-analysis\tAnalyse TikTok hashtags Python\tTikTok\tCode blog-webscraping\tWebscraping using rvest and selenium R\tBlogs\tCode twitter-streaming\tLarge-scale data collection on X (Twitter) Python\tTwitter / X\tCode twitter-social-network\tSocial Network Analysis with R R\tTwitter / X\tCode  ","version":"Next","tagName":"h3"},{"title":"Design principles​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#design-principles","content":" The editorial team has adopted four guiding principles for content on the Data Knowledge Hub:  From general to specific: Cater to different target groups by starting each chapter with a general and easy-to-follow introduction. More specific topics such as content on use cases, projects, or code examples will be added throughout the project. We use three labels to indicate difficulty that will help users to orientate themselves: no code, beginners, advanced.Rich links: Enable non-linear interaction with internal and external links, highlighting diverse initiatives, projects, or code libraries.Reproducibility: For code examples, we focus on Python and R due to their widespread use in data science (however use cases in other languages are also welcome such as JavaScript, Julia or Rust). All code should be reproducible.Open Source: Content and code will be accessible on GitHub under a CC BY License.  ","version":"Next","tagName":"h3"},{"title":"Structure of the Data Knowledge Hub​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#structure-of-the-data-knowledge-hub","content":" The structure of the hub is based on the different stages of a data analysis project:  How to get started: Overview of legal and ethical considerations as well as tools you may use during your project.How to access data on platforms: Information on data access options available for each platform.How to collect data: Summary of data collection methods and tools, along with challenges, limitations, and potential.How to analyse data: Introduction to research designs and methods like natural language processing, network analysis, and machine learning.Literature and Illustrative Research: Overview of literature and selected research and studies.Contribute: Now it's your turn. Information on how you can support us and make your research available to the data knowledge community.  ","version":"Next","tagName":"h3"},{"title":"Questions and improvements​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#questions-and-improvements","content":" If you have any questions or ideas, please do not hesitate to contact us at upgrade.democracy@bertelsmann-stiftung.de.  ","version":"Next","tagName":"h3"},{"title":"License​","type":1,"pageTitle":"The Data Knowledge Hub for Researching Online Discourse","url":"/data-knowledge-hub/docs/background-rationale#license","content":"   This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ","version":"Next","tagName":"h3"},{"title":"Roadmap","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/contribute/06_04_roadmap","content":"","keywords":"","version":"Next"},{"title":"Releases​","type":1,"pageTitle":"Roadmap","url":"/data-knowledge-hub/docs/contribute/06_04_roadmap#releases","content":" ","version":"Next","tagName":"h2"},{"title":"Version 1.3 (Q4 2024)​","type":1,"pageTitle":"Roadmap","url":"/data-knowledge-hub/docs/contribute/06_04_roadmap#version-13-q4-2024","content":"  Add more ways to contribute to the Data Knowledge Hub New feature for more dynamic contribution panels Collect user feedback and update content library for a better user journey  Open for contributions We welcome contributions on a rolling basis. Have you done an anlysis or project you want to share? Get in touch with us at upgrade.democracy@bertelsmann-stiftung.de  ","version":"Next","tagName":"h3"},{"title":"Version 1.2 (Q3 2024)​","type":1,"pageTitle":"Roadmap","url":"/data-knowledge-hub/docs/contribute/06_04_roadmap#version-12-q3-2024","content":"  Release of open source repository with Data Knowledge Hub Projects  ","version":"Next","tagName":"h3"},{"title":"Version 1.1 (Q1 2024)​","type":1,"pageTitle":"Roadmap","url":"/data-knowledge-hub/docs/contribute/06_04_roadmap#version-11-q1-2024","content":"  Add brief tutorials and/or explainer videos to improve accessibility of content  ","version":"Next","tagName":"h3"},{"title":"Version 1.0 (Q4 2023)​","type":1,"pageTitle":"Roadmap","url":"/data-knowledge-hub/docs/contribute/06_04_roadmap#version-10-q4-2023","content":"  Publish all chapters of the initital Data Knowledge Hub Fixed some formatting issues  ","version":"Next","tagName":"h3"},{"title":"Version 0.9 (Q3 2023)​","type":1,"pageTitle":"Roadmap","url":"/data-knowledge-hub/docs/contribute/06_04_roadmap#version-09-q3-2023","content":"  Publish the Data Knowledge Hub Initial content from expert contributors Initial content from the Upgrade Democracy team Initial design of the Data Knowledge Hub ","version":"Next","tagName":"h3"},{"title":"How to access data on platforms","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-access/","content":"How to access data on platforms Once you're familiar with the relevant ethical considerations and legal framework for researching online discourse, the next step is accessing the actual data. To support your research, we’ve compiled an overview of the data access options available for each platform, helping you navigate the specific processes and policies that govern how data can be retrieved and used: Overview: Access to data for independent research: This chapter provides an introduction to accessing data for social media monitoring through the lens of platform regulations and policies. Open for contributions We welcome contributions on a rolling basis. At the moment, we particularly welcome chapters dealing with the following questions How to deal with &quot;dark socials&quot;?Data access rights beyond the European Union and the U.S.","keywords":"","version":"Next"},{"title":"Guide: How to format content using Markdown","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/contribute/06_03_markdown-features","content":"","keywords":"","version":"Next"},{"title":"Front Matter​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/data-knowledge-hub/docs/contribute/06_03_markdown-features#front-matter","content":" Markdown documents have metadata at the top called Front Matter:  my-doc.md --- id: my-doc-id title: My document title description: My document description slug: /my-custom-url --- ## Markdown heading Markdown text with [links](./hello.md)   ","version":"Next","tagName":"h2"},{"title":"Links​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/data-knowledge-hub/docs/contribute/06_03_markdown-features#links","content":" Regular Markdown links are supported, using url paths or relative file paths.  Let's see how to [see &quot;Contribute&quot; section](06_01_how-to-contribute).   Let's see how to [see &quot;Contribute&quot; section](../06_01_how-to-contribute).   Result: Let's see how to see &quot;Contribute&quot; section.  ","version":"Next","tagName":"h2"},{"title":"Images​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/data-knowledge-hub/docs/contribute/06_03_markdown-features#images","content":" Regular Markdown images are supported.  Add an image at static/img/docusaurus.png and display it in Markdown:  ![Docusaurus logo](/img/docusaurus.png)     ","version":"Next","tagName":"h2"},{"title":"Code Blocks​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/data-knowledge-hub/docs/contribute/06_03_markdown-features#code-blocks","content":" Markdown code blocks are supported with Syntax highlighting.  src/components/HelloDocusaurus.js function HelloDocusaurus() { return ( &lt;h1&gt;Hello, Docusaurus!&lt;/h1&gt; ) }   src/components/HelloDocusaurus.js function HelloDocusaurus() { return &lt;h1&gt;Hello, Docusaurus!&lt;/h1&gt;; }   ","version":"Next","tagName":"h2"},{"title":"Admonitions​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/data-knowledge-hub/docs/contribute/06_03_markdown-features#admonitions","content":" Docusaurus has a special syntax to create admonitions and callouts:  My tip Use this awesome feature option  Take care This action is dangerous  My tip Use this awesome feature option  Take care This action is dangerous  ","version":"Next","tagName":"h2"},{"title":"MDX and React Components​","type":1,"pageTitle":"Guide: How to format content using Markdown","url":"/data-knowledge-hub/docs/contribute/06_03_markdown-features#mdx-and-react-components","content":" MDX can make your documentation more interactive and allows using any React components inside Markdown:  export const Highlight = ({children, color}) =&gt; ( &lt;span style={{ backgroundColor: color, borderRadius: '20px', color: '#fff', padding: '10px', cursor: 'pointer', }} onClick={() =&gt; { alert(`You clicked the color ${color} with label ${children}`) }}&gt; {children} &lt;/span&gt; ); This is &lt;Highlight color=&quot;#25c2a0&quot;&gt;Docusaurus green&lt;/Highlight&gt; ! This is &lt;Highlight color=&quot;#1877F2&quot;&gt;Facebook blue&lt;/Highlight&gt; !     This is Docusaurus green !  This is Facebook blue ! ","version":"Next","tagName":"h2"},{"title":"How to access data for social media monitoring: availability, limitations, and outlook","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-access/02_02_overview-access","content":"","keywords":"","version":"Next"},{"title":"Introduction: A matter of access​","type":1,"pageTitle":"How to access data for social media monitoring: availability, limitations, and outlook","url":"/data-knowledge-hub/docs/data-access/02_02_overview-access#introduction-a-matter-of-access","content":" Social media is integral to modern communication and democratic discourse. It reveals us to ourselves, documenting our emotions, opinions, political stances, and behaviors, albeit often to an extreme and distorted extent. Since the rise of social media, researchers from varied disciplines and institutions have sought to study it and its impact on, or reflection of, society.  And yet, researchers, particularly non-academic and independent researchers have struggled for meaningful research access to platform data. A variety of researchers can only serve public interest by varying available analyses and results by length of study, research scope, context, and language, to name a few.  After years of debate, the European Union’s Digital Services Act (DSA) entered into force on August 25th, 2023. It begins the work of regulating Big Tech, particularly social media platforms. Among other imperatives, the DSA addresses the existing information asymmetry between the public and VLOPs (very large online platforms) by compelling them to grant data access to researchers (see Article 40, Data access and scrutiny).  Importantly, VLOPs are no longer the arbiters of data access. Digital Service Coordinators (DSCs), designated by each Member State, are tasked to vet independent researchers for access. This separation from platform control will allow for research from a critical standpoint on the platforms’ design and the occurrence of ‘systemic risks’ (DSA Article 34).  This opens more opportunities for civil society to participate in quantitative social media research with fewer barriers to access. By analyzing the engagement, sentiment, and reach of content on social platforms, non-academic researchers can gauge ongoing and evolving public sentiment and empower policymakers to make informed decisions by responding actively to changing social dynamics.  ","version":"Next","tagName":"h2"},{"title":"Availability: What can you access and how?​","type":1,"pageTitle":"How to access data for social media monitoring: availability, limitations, and outlook","url":"/data-knowledge-hub/docs/data-access/02_02_overview-access#availability-what-can-you-access-and-how","content":" The avenues of access fall under two basic categories:  through crowd listening tools, such as CrowdTangle provided for Meta products, andapplication programming interfaces, or APIs. APIs require coding skills, but their use allows researchers more flexibility in the way they can gather data and to gather it more precisely.  All major social media platforms (Meta, Twitter/X, TikTok, YouTube, and Telegram) offer API access, though some have accessibility restrictions. TikTok, for instance, only offers an API for developers. A parallel TikTok API for researchers is only available in the United States, leaving European institutions in the dark.  Here is an overview of data access by each platform. See more details in DRI’s data access series.    Overview of data access by each platform  ","version":"Next","tagName":"h2"},{"title":"Limitations: What’s missing?​","type":1,"pageTitle":"How to access data for social media monitoring: availability, limitations, and outlook","url":"/data-knowledge-hub/docs/data-access/02_02_overview-access#limitations-whats-missing","content":" If the DSA is now in force, mandating access to data, everything is solved, right? Not so much. There are still limitations, ambiguity, and technicalities that need to be ironed out.  It is not yet clear how new researchers will be vetted. While the DSA has described the ultimate direction, the practical implementation is left to what is called a ‘Delegated Act’. In response, civil society has responded with recommendations and public petitions. At DRI, ours include providing access to all public data, regular updates to APIs, granting access to non-academic researchers, not just developers, and better-informed Terms of Service adhering to the DSA.  In the meantime, many platforms currently still limit the type of data accessible through their APIs, even data that is public in nature. The primary blind spots are features like the comment sections under posts (which notoriously tend to hold more problematic or illegal content than the post itself), profiles set to ‘public’ on Facebook, new features such as ‘Stories’. See below for a breakdown of the type of data you can access by platform.    Overview of the type of data accessible by platform  The biggest gap now comes from the newly minted X.com (the artist formerly known as Twitter). Earlier this year, X imposed a paywall for its API access which is prohibitively expensive (the lowest, and most limited tier in terms of quantity of data, is 100 dollars per month). This year X has left a trail of non-compliance both with the impending DSA and the voluntary Code of Practice on Disinformation, representing a stark contrast to the changing tide. The platform will likely face fines from the EU for its actions.  ","version":"Next","tagName":"h2"},{"title":"Outlook: What can you do now (including in terms of advocacy)?​","type":1,"pageTitle":"How to access data for social media monitoring: availability, limitations, and outlook","url":"/data-knowledge-hub/docs/data-access/02_02_overview-access#outlook-what-can-you-do-now-including-in-terms-of-advocacy","content":" The Delegated Act detailing the new provisions for vetting and securing access are expected to be released in early 2024. In the meantime, social media research is still a vital area of research.  If you are interested in improving research access and social media research on global online political discourse, sign up for our newsletter, The Digital Drop. ","version":"Next","tagName":"h2"},{"title":"How to analyse data","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-analysis/","content":"How to analyse data To help you get started and inspired with social media analyses, we’ve collected a first set of practical projects that provide a starting point. There are a myrdiad of projects and methods discussion around social media data. Therefore, we welcome additional content and have added a selection of chapters below that would make for great contributions. Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton: In this chapter you will learn about the different steps necessary to conduct a hashtag analysis on TikTok: From collecting the data to analysing it with Python and visualising it. Introduction to social network analysis in R and Gephi: This chapter is an introductory tutorial to the broad field of social network analysis. The tutorial includes an example of how to use social network analysis to detect communities in online debates on social platforms like X (Twitter). Open for contributions We welcome contributions on a rolling basis. Have you done an analysis or a research project you want to share? Do you want to contribute a primer for a specific method? Get in touch with us at upgrade.democracy@bertelsmann-stiftung.de","keywords":"","version":"Next"},{"title":"How to collect data on platforms","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-collection/","content":"How to collect data on platforms Data Collection on social media and digital platforms comes with a range of specificities and nuances that, unfortunately, differ across each platform. To facilitate research and give you an idea of what’s possible on which platform, this section outlines data collection approaches and introduces examples for TikTok, X (Twitter), and blogs (including code). Additional examples or contributions to data collections methods are welcome, suggestions are listed under (4) call for contributions. Overview of Data Collection Methods on Social Media Platforms: Focusing on TikTok as a case study, this chapter offers insights into the myriad ways one can audit an online platform. It underscores the importance of aligning the chosen method with the research question at hand. Drawing on our experiences with auditing recommender systems, this chapter presents a holistic understanding of TikTok’s practices. Data Collection on X (Twitter): X (Twitter) with its vast user base and real-time data, has always been a fertile ground for researchers and developers alike. This chapter provides a comprehensive guide to making the most out of the API, with specific attention to a newly developed Python library, underscoring its flexibility and scalability. Webscraping Techniques with R: In a constantly changing digital environment, adaptability is key. As various social networks have begun to limit access to their APIs—either monetising them or closing them altogether — researchers face the challenge of capturing crucial data. The significance of webscraping has thus resurged, offering an alternative means of data collection. Open for contributions Contributions are welcome, particularly case studies on Facebook, Instagram, YouTube and other platforms as well as additional data collection tactics and methods.","keywords":"","version":"Next"},{"title":"How to stream, store and retrieve data on X (Twitter)","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/","content":"How to stream, store and retrieve data on X (Twitter) Level required: Beginner Platform: X Language: Python AN Andreas Neumeier SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn JR Jasmin Riedl SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn WD Wiebke Drews SPARTA | University of the German Federal Armed Forces Website Twitter / X Mastodon Google Scholar LinkedIn Social media platforms have gained significant importance in recent years due to their extensive reach and high usage. They provide researchers in various fields with valuable insights into public, social, and political areas, making them an essential data source. When it comes to X (formerly Twitter) and its data gathering, specific features exist, which we discuss in the following documents. The provided documents outline the management of X (Twitter) data and guide users through data streaming, storage, and retrieval. The streaming-database document recommends methods for working with X (Twitter) streaming data and covers storage in both relational and non-relational databases. The twitter-api page introduces the Python implementation of Twitter API v2 through the sparta-twitterapi package. The guide outlines the prerequisites for X (Twitter), including required software, tools, and developer credentials, and provides clear instructions on installing the Python package and completing initial authentication. Additionally, the document describes the available endpoints through the Python package. The twitter-rules guide demonstrates how to utilize X’s API tools for searching and filtering. It assists users in conducting efficient searches and explains the operators and their functions.","keywords":"","version":"Next"},{"title":"Example: X (Twitter) API","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api","content":"","keywords":"","version":"Next"},{"title":"How to access data via the Twitter API v2​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#how-to-access-data-via-the-twitter-api-v2","content":" Developers can access a wide range of X (Twitter) data, including Tweets, users, and more features with the Twitter API v2. The API provides vast opportunities, whether to gather insights, build a new application, or improve an existing one. Version 2 of the Twitter API offers a more flexible and scalable approach to accessing Twitter data, and the Python implementation aims to simplify this process for Python developers. Documentation for the Python package is available at https://unibwsparta.github.io/twitterapi/index.html.  ","version":"Next","tagName":"h2"},{"title":"1. Prerequisites​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#1-prerequisites","content":" ","version":"Next","tagName":"h2"},{"title":"1.1. Software & Tools​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#11-software--tools","content":" Python (3.8 or higher)Pip (Python package installer)  ","version":"Next","tagName":"h3"},{"title":"1.2 Twitter Credentials​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#12-twitter-credentials","content":" To use the Twitter API, you must have a developer account on Twitter, where you'll obtain the Bearer Token.  ","version":"Next","tagName":"h3"},{"title":"2. Installation​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#2-installation","content":" Install the required Python package using pip:  pip3 install sparta-twitterapi   ","version":"Next","tagName":"h2"},{"title":"3. Basic Usage​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#3-basic-usage","content":" ","version":"Next","tagName":"h2"},{"title":"3.1 Authentication​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#31-authentication","content":" Authentication is required before making any requests to the API. The library reads the Bearer Token from an environment variable named BEARER_TOKEN. You can set the environment variable directly or through the os library in the Python code before the imports.  Set the environment variable in bash:  export BEARER_TOKEN=&quot;xxxxxxxxxxx&quot;   Set the environment variable in python:  import os os.environ[&quot;BEARER_TOKEN&quot;] = &quot;xxxxxxxxxxx&quot;   ","version":"Next","tagName":"h3"},{"title":"3.2 First example​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#32-first-example","content":" This script retrieves two tweets defined by the tweet ID and outputs their content on the console.  import os os.environ[&quot;BEARER_TOKEN&quot;] = &quot;XXXXXXXXXXXXXX&quot; from sparta.twitterapi.tweets.tweets import get_tweets_by_id async for tweet_response in get_tweets_by_id(['1511275800758300675', '1546866845180887040']): print(tweet_response.tweet)   ","version":"Next","tagName":"h3"},{"title":"4. Available Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#4-available-endpoints","content":" The sparta-twitterapi package provides several endpoints organized into three core categories: These consist of Compliance, Users, and Tweets. It also offers a data model for obtaining tweets that is resistant to API changes. Here’s a comprehensive overview. For further details, refer to the official documentation.  ","version":"Next","tagName":"h2"},{"title":"4.1 Data Model​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#41-data-model","content":" To account for differences between X’s (Twitter’s) API description and its actual responses, we devised a Pydantic model to represent tweet responses as the original classes provided by X (Twitter) couldn’t be used. Every tweet includes two attributes. tweet_response.tweet comprises the original tweet object, including details about the tweet, such as its id, creation date, text, entities, and more. The extension objects linked with the tweet, such as retweeted tweets, quoted tweets, users, media, polls, and places, are included in tweets_response.includes. Find all tweet attributes and their corresponding descriptions here.  ","version":"Next","tagName":"h3"},{"title":"4.2 Tweets Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#42-tweets-endpoints","content":" The twitterapi package provides various endpoints in the &quot;Tweets&quot; category that serve different functions and requirements associated with Twitter posts. Developers can utilize these endpoints related to Tweets to obtain comprehensive tools for harnessing, analyzing, and managing the vast amount of information flowing through Twitter every moment.  In the endpoints provided by the Python library, all available fields and extensions are retrieved by default. For comprehensive details, parameter explanations, and best practices, you should refer to the official sparta-twitterapi package documentation. Here is a thorough outline of the endpoints that are available in this category:  Filtered Stream Endpoint​  The filtered stream endpoint group is designed for developers who wish to modify the public tweets' real-time stream based on specific criteria. This endpoint group allows users to follow real-time conversations on specific topics or events and observe trend development. This group includes several endpoints allowing:  Creating and managing rules. Apply a set of rules to filter real-time tweets and return only the tweets that fulfill the criteria.Connect to the filtered stream.  Full-archive Search Endpoint​  The full-archive search endpoint is available exclusively to projects with Academic Research or Enterprise access. It provides the capability to:  Access all public Tweets from the archive, starting with the very first Tweet in March 2006.Retrieve them based on specific search queries.  Quote Tweets Lookup Endpoint​  Retrieving Quote Tweets that are linked to a specific Tweet ID. This allows developers and researchers to quickly extract the entire conversation related to a certain Tweet, along with all its Quote Tweets, which promotes a thorough comprehension of the discussions.  Recent Search Endpoint​  The recent search endpoint provides access to:  Filtered public tweets posted in the last week.  Retweeted By Endpoint​  Users can utilize the Retweets lookup endpoint to:  Obtain a list of accounts that retweeted a specific Tweet.  Tweets Endpoint​  This set of endpoints is designed to be simple, offering methods to:  Retrieve either a single Tweet or a group of Tweets identified by a Tweet ID. Despite the simplicity of this endpoint, it's essential for various tasks, such as retrieving updated Tweet details, checking a Tweet's availability, reviewing its edit history, and managing compliance events.  ","version":"Next","tagName":"h3"},{"title":"4.3 Users Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#43-users-endpoints","content":" The twitterapi package provides various endpoints under the 'Users' category. These endpoints are designed to offer insights and data about Twitter users. Below is a detailed breakdown of the available endpoints in this category:  Followers Lookup Endpoint​  The Followers Lookup Endpoint is designed to unravel and analyze the relationships between Twitter users. This process is commonly referred to as network analysis. There are two primary endpoints in this category:  get_following_by_id: This endpoint returns user objects that represent the users that a specific user is following. It provides a list of users whom the queried user has chosen to follow. get_followers_by_id: On the other hand, this endpoint returns user objects that represent the users who follow the specified user. It gives a list of users who follow the queried user.  Both of these endpoints provide valuable insights into the connection dynamics of specific accounts by shedding light on user relationships.  User Lookup Endpoint​  The User Lookup Endpoint is a service that retrieves details of one or more users based on either a user ID (get_users_by_ids) or username (get_users_by_username). This endpoint uses the GET method to return the following:  User Details: The response typically includes user objects that contain fields such as follower count, location, pinned Tweet ID, and profile bio, among others. These fields provide a comprehensive view of the user's public profile and preferences. Find all user attributes and their corresponding descriptions here.  Developers, who seek to gain insights into user behavior, relationships, and characteristics on Twitter, will find these Users endpoints particularly useful. Consult the official documentation of the twitterapi package to gain a more detailed comprehension and explore other preferences and criteria.  ","version":"Next","tagName":"h3"},{"title":"4.4 Compliance Endpoints​","type":1,"pageTitle":"Example: X (Twitter) API","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-api#44-compliance-endpoints","content":" For ensuring compliance with events, developers, who store X (Twitter) data offline, must reflect the real-time state and user intent on X (Twitter). This involves updating the data whenever users delete or edit their tweets, protect their accounts, or make other changes. There are two methods to perform this task:  Compliance Event Stream​  The Compliance Event Stream provides real-time updates on any compliance-related events, ensuring that any stored data is kept in sync with the live state on X (Twitter).  Batch Compliance Mode​  Batch compliance endpoints are valuable for developers managing extensive datasets. By uploading sizable collections of Tweet or user IDs, you can quickly determine the compliance status of each entry. Identifying data that requires action ensures the compliance of your datasets. It is important to note that the batch compliance endpoints are specifically designed for their intended purposes. Any use of these endpoints outside of their intended purposes is strictly prohibited and could result in enforcement actions taken by X (Twitter). ","version":"Next","tagName":"h3"},{"title":"Best Practices to Stream and Store Data on X","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/streaming-database","content":"","keywords":"","version":"Next"},{"title":"How to Deal with Varying Amounts of X Streaming Data​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/streaming-database#how-to-deal-with-varying-amounts-of-x-streaming-data","content":" Expect the number of tweets coming through the filtered stream to vary based on factors like time of day or current events. This fluctuation means that processing capabilities must be adaptable to handle increases or decreases in volume. Additionally, it’s important to note that X’s (Twitter’s) filtered stream endpoint does not include a built-in buffer. Therefore, if incoming data is not processed promptly, the stream may stop working.  If you experience a disconnection, you may retrieve data up to five minutes after the interruption. However, it is recommended to minimize interruptions to maintain a consistent and efficient data flow. One effective approach is establishing a local buffer to temporarily store incoming tweets for later processing.  Moreover, incorporating a publish-subscribe system like Kafka is worth considering. With this system in place, incoming tweets are stored instantly, allowing subsequent processes to access and analyze them at their own pace. This approach effectively manages peak loads and balances the data processing workload.  Always monitor your analysis pipeline’s capacity. If you notice that it is struggling to handle the regular tweet volume, it is time to consider scaling up your resources. This step ensures that your system stays reliable and responsive, especially during times of high demand.  ","version":"Next","tagName":"h2"},{"title":"How to Store your X Data​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/streaming-database#how-to-store-your-x-data","content":" To ensure that tweets can be processed later, they need to be stored in a structured format.  The method in which the data is stored varies depending on the specific application. It is feasible to store them in both relational and non-relational databases.  For effective storage and retrieval of tweets, selecting the appropriate database structure is crucial. Your chosen structure depends heavily on your application’s nature and how you intend to interact with stored data. Here’s a quick overview of how you can store and/or organise tweets in both relational and non-relational databases:  ","version":"Next","tagName":"h2"},{"title":"1. Relational Databases​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/streaming-database#1-relational-databases","content":" In a relational database (e.g., PostgreSQL, MySQL), information is arranged into tables with predetermined columns, and connections between these tables are clearly defined through primary and foreign keys.  tip Remember that the decision to use a relational database such as PostgreSQL should depend on your application’s requirements. If your application relies heavily on structured queries and data relationships, a relational database like PostgreSQL can be very beneficial.  PostgreSQL-Schema example for tweets:  id (text)​  Description: A unique identifier for each tweet.Constraints: Must be unique for each tweet.Example: &quot;1234567890&quot;  project (text)​  Description: The name of a project. This allows you to store tweets from multiple projects in the same table.Example: &quot;project1&quot;  created_at (datetime)​  Description: The date and time when the tweet was published.Constraints: Must be a valid date and time.Example: 2023-08-24 12:34:56+00  author_id (text)​  Description: The unique identifier of the user who authored the tweet.Example: &quot;9876543210&quot;  text (text)​  Description: The actual content of the tweet, limited to a specific number of characters.Example: &quot;This is a sample tweet!&quot;  retweeted_id (text)​  Description: If the tweet is a retweet, this field stores the original tweet's unique identifier.Constraints: Should map to a valid tweet id or be NULL if it's not a retweet.Example: &quot;987654321&quot;  conversation_id (text)​  Description: An identifier to group tweets from the same conversation/thread.Constraints: Can be same as the tweet's id if it's the start of a conversation or can map to another tweet's id if it's part of an ongoing conversation.Example: &quot;1122334455&quot;  media (jsonb)​  Description: A JSON object containing information about any media (like images, videos) attached to the tweet.Example: [ { &quot;url&quot;: &quot;https://pbs.twimg.com/media/test.jpg&quot;, &quot;type&quot;: &quot;photo&quot;, &quot;width&quot;: 4086, &quot;height&quot;: 3065, &quot;media_key&quot;: &quot;3_123456&quot;, &quot;public_metrics&quot;: {} } ]   polls (jsonb)​  Description: A JSON object containing data about any polls included in the tweet.Example: [ { &quot;id&quot;: &quot;123456789&quot;, &quot;question&quot;: &quot;Favorite color?&quot;, &quot;options&quot;: [ { &quot;label&quot;: &quot;Red&quot;, &quot;votes&quot;: 18, &quot;position&quot;: 1 }, { &quot;label&quot;: &quot;Blue&quot;, &quot;votes&quot;: 28, &quot;position&quot;: 2 }, { &quot;label&quot;: &quot;Green&quot;, &quot;votes&quot;: 18, &quot;position&quot;: 3 } ], &quot;end_datetime&quot;: &quot;2023-08-24 12:34:56.000Z&quot;, &quot;voting_status&quot;: &quot;closed&quot;, &quot;duration_minutes&quot;: 10080 } ]   places (jsonb)​  Description: A JSON object containing geolocation or place information associated with the tweet.Constraints: Should adhere to a pre-defined structure for storing place data.Example: [ { &quot;id&quot;: &quot;37439688c6302728&quot;, &quot;geo&quot;: { &quot;bbox&quot;: [11.360589, 48.061634, 11.722918, 48.248124], &quot;type&quot;: &quot;Feature&quot;, &quot;properties&quot;: {} }, &quot;name&quot;: &quot;Munich&quot;, &quot;country&quot;: &quot;Germany&quot;, &quot;full_name&quot;: &quot;Munich, Germany&quot;, &quot;place_type&quot;: &quot;city&quot;, &quot;country_code&quot;: &quot;DE&quot; } ]   entities (jsonb)​  Description: A JSON object that captures specific entities within the tweet like hashtags, user mentions, URLs, etc.Constraints: Should adhere to a pre-defined structure for capturing these entities.Example: { &quot;hashtags&quot;: [ { &quot;end&quot;: 15, &quot;start&quot;: 0, &quot;tag&quot;: &quot;#example&quot; }, { &quot;end&quot;: 30, &quot;start&quot;: 15, &quot;tag&quot;: &quot;#database&quot; } ], &quot;mentions&quot;: [ { &quot;id&quot;: &quot;123456&quot;, &quot;end&quot;: 15, &quot;start&quot;: 0, &quot;username&quot;: &quot;@username&quot; } ] }   tweet (jsonb)​  Description: A JSON object that captures the entire raw tweet data, useful in case something changes in the Twitter API, for backup, or for applications that need the full payload.Example: {&quot;id&quot;: &quot;1234567890&quot;, &quot;text&quot;: &quot;This is a sample tweet!&quot;, &quot;lang&quot;: &quot;de&quot;, &quot;public_metrics&quot;: {&quot;like_count&quot;: 0, &quot;quote_count&quot;: 0, &quot;reply_count&quot;: 0, &quot;retweet_count&quot;: 0, &quot;impression_count&quot;: 0}, ...}   compliance (text)​  Description: Displays the compliance status of the tweet in terms of compliance events such as deleted, edited, witheld. This can be used to indicate whether a compliance event exists for a tweet.Example: deleted  ","version":"Next","tagName":"h3"},{"title":"2. Non-Relational Databases​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/streaming-database#2-non-relational-databases","content":" Non-relational databases (e.g., MongoDB, Cassandra) offer greater storage flexibility since they don’t need a fixed schema.  Document-based storage (like MongoDB) example:  Each tweet can be a document:  { &quot;_id&quot;: &quot;1234567890&quot;, &quot;author_id&quot;: &quot;9876543210&quot;, &quot;text&quot;: &quot;This is a sample tweet!&quot;, &quot;created_at&quot;: &quot;2023-08-24 12:34:56+00&quot;, &quot;entities&quot;: { &quot;hashtags&quot;: [ {&quot;end&quot;: 15, &quot;start&quot;: 0, &quot;tag&quot;: &quot;#example&quot;}, {&quot;end&quot;: 30, &quot;start&quot;: 15, &quot;tag&quot;: &quot;#database&quot;} ], &quot;mentions&quot;: [ {&quot;id&quot;: &quot;123456&quot;, &quot;end&quot;: 15, &quot;start&quot;: 0, &quot;username&quot;: &quot;@username&quot;} ] }, ... }   Non-relational databases excel when:  The data structure is flexible and might change over time.Horizontal scalability is required.High write speeds are necessary, and data consistency can be slightly relaxed.  ","version":"Next","tagName":"h3"},{"title":"In Short: How to choose your database​","type":1,"pageTitle":"Best Practices to Stream and Store Data on X","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/streaming-database#in-short-how-to-choose-your-database","content":" The choice between relational and non-relational databases should be made based on your application’s specific requirements. If the application heavily relies on structured relationships, complex queries, and data integrity, a relational database is preferable. On the other hand, if your application needs scalability, flexibility in schema design, and can compromise a bit on consistency, a non-relational database might be the better choice. ","version":"Next","tagName":"h2"},{"title":"Introduction to social network analysis in R and Gephi","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis","content":"","keywords":"","version":"Next"},{"title":"Where to start: An introduction to Social Network Analysis​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#where-to-start-an-introduction-to-social-network-analysis","content":" Social network analysis (SNA) is a method used to study and better understand the relationships and interactions among individuals, groups, or entities. In social network analysis, you represent these relationships as a network, where each individual or entity is a node, and the connections between them are represented as edges. Depending on the terminology sometimes nodes are also called vertices.  SNA can help you uncover valuable insights about how information, resources, and influence flow within networked systems. It allows you to analyze patterns, identify key individuals or groups, and understand the overall structure and dynamics of a social network.  Early examples from sociology for instance, identified patterns of the spread of sexual diseases at high schools. Today in a digitalized world, human behavior leaves traces in many ways whether we write messages to someone on X (Twitter) or WhatsApp or walk through the city with our mobile phone. This certainly raises privacy concerns that are addressed by data protection regulations like the European Union’s GDPR. Privacy concerns and further ethical consideration need to be considered during all steps of a research project; when designing the methodological approach, during data collection and analysis as well as during the writing and publication process. Now, let’s consider a few examples of how social network analysis can be applied on real-world data:  Online Social Networks: Imagine you have data from a social media platform like Facebook or X (Twitter). You can use social network analysis in R to study patterns of communication, identify influential users, detect communities, and understand information diffusion. For instance, you can analyze how ideas or trends spread across a network of X (Twitter) users.Collaboration Networks: In academic or professional settings, social network analysis can be used to study collaboration patterns among researchers, employees, or organizations. For instance, each mention of another research creates a link to the author and the referenced article (or the article URL). By analyzing co-authorship networks or co-worker networks, you can identify key players, measure the impact of collaborations, and uncover potential research or business opportunities.Organizational Networks: Social network analysis can also be applied to understand the structure and dynamics of organizations. By examining communication networks within a company, you can identify bottlenecks, assess information flow, and evaluate the effectiveness of teams or departments. This analysis can help organizations optimize their structure and improve collaboration. This, however, comes with high ethical and privacy concerns, and you should ensure that your research is covered by employment or data protection laws.  In this tutorial, you will primarily learn about community detection on X (Twitter), how to analyze its networked structure, detect community structures based on users’ communication behavior. Some measures and terms that you will hear frequently and help to assess and understand network structures are the following:  Degree Centrality: The degree of a node is the number of other nodes that a node is connected to. Important nodes tend to have more connections to other nodes. Highly connected nodes are interpreted to have high degree centrality. In the later case of X (Twitter) “retweet networks” a high degree of incoming edges (indegree) represents being retweeted frequently and can be used to identify influential accounts in the given debate. The number of outgoing edges (outdegree) represents the number of times a given account or user retweeted other accounts. Regarding, retweet behavior, this may indicate partisan support or if the number is very high potentially inauthentic behavior of an account (e.g. through bots). Eigenvector Centrality: The extent to which adjacent nodes are connected also indicate importance (out differently: important nodes increase the importance of other nodes). This measure basically assesses how connected a node is on higher degrees of connection such as second and third-degree connections. Closeness centrality: Closeness centrality measures how many steps are required to access every other node from a given node. In other words, nodes with a high closeness centrality have easy access to other nodes given multiple connections. Betweenness Centrality: The betweenness centrality measure ranks the nodes based on the flow of connections through the network. Importance is demonstrated through high frequency of connection with multiple other nodes. Nodes with high levels of betweenness tend to serve as a bridge for multiple sets of other important nodes. Modularity: Modularity is a measure of the structure of networks which measures the strength of segments of a network into communities (also called groups, clusters or modules). Networks with high modularity have relatively densely connected components, that means there are dense connections between the nodes within communities but sparse connections between nodes in different communities (Newman 2006). Modularity is often used in optimization methods for detecting community structure in networks such as the Louvain algorithm (Blondel et al. 2008).  ","version":"Next","tagName":"h2"},{"title":"Getting started with social network analysis​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#getting-started-with-social-network-analysis","content":" ","version":"Next","tagName":"h2"},{"title":"Your setup​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#your-setup","content":" To perform social network analysis in R or Python, you can use packages and libraries like igraph, network, and sna. These packages provide functions to import network data, create network objects, visualize networks, and analyze network properties such as centrality measures (e.g., degree, betweenness) or community detection. By applying these techniques, you can gain sometimes mesmerizing insights into the social relationships and interactions that shape our lives and communities. Social network analysis in R allows you to explore, visualize, and understand complex social networks, ultimately helping you make informed decisions, improve social systems and in this tutorial’s example better understand large-scale political discourses online and the potential amplification fo false information.  ","version":"Next","tagName":"h3"},{"title":"Recommended reading​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#recommended-reading","content":" Remember, social network analysis is a vast field with numerous advanced methods and techniques. So, if you find it intriguing, there is a lot more to explore and learn! As introductory books I would recommend Carrington et al. (2005), Scott (2017) or Yang (2016). Other scholars and online researchers have also created tutorials introducing network analysis and its application based on a variety of tools from programming and data analysis software like R and Python to open-source OSINT tools like Gephi.  ","version":"Next","tagName":"h3"},{"title":"Parameters of example case​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#parameters-of-example-case","content":" The following tutorial is based on X (Twitter) data that was accessible for researchers via X’s (Twitter’s) Academic API until February 2023 when Elon Musk and Twitter closed the free Research API access to X’s (Twitter) data. The data used for this tutorial was collected using an R package called AcademicTwitter developed by colleagues from the research community.  Considering the implementation of the EU Digital Services Act, we can expect extended research data access for so-called vetted researchers at the latest by September 2024. Moreover, there should be APIs with limited research data access for a broad research community. Here you can find a policy brief arguing for the importance of research data access for policymaking and better understanding social implications of digital online platforms and search engines (https://opus4.kobv.de/opus4-hsog/frontdoor/deliver/index/docId/4947/file/Implementing_Data_Access_Darius_Stockmann_2023.pdf).  ","version":"Next","tagName":"h3"},{"title":"Step-by-Step tutorial on X (Twitter) networks in R​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#step-by-step-tutorial-on-x-twitter-networks-in-r","content":" But now let’s get started creating and analyzing our first X (Twitter) networks!  As selection criteria for your data collection, it is possible to filter for key words, hashtags or collect the timelines (messages shared) by users via user lists. This is mostly used for political elites like politicians, new network elites like political influencers or profiles of media outlets (whatever type of accounts you are interested in).  To illustrate this work, this tutorial replicated the work from Darius and Urquhart (2021), in which we collected data by choosing hashtags as selection criteria. When interested in information diffusion, the spread of information and messages on social networking sites like X (Twitter), retweeting is the most often used mechanism to amplify or share messages by other users. Retweeting also creates a link (edge) between two accounts (nodes), constituting a retweet network. These retweet networks often cluster into multiple communities, and for political hashtags, these communities represent different ideologies or opinions on a topic (Conover et al., 2012).  Thus, retweet networks provide the chance to assess ideological alignment and opinion leaders/influencers within communities where people self-sort via their retweeting behavior (Conover et al., 2012; Bruns et al., 2016). This self-sorting into different aligned communities happens because most users retweet in support of messages (Boyd et al., 2010; Metaxas et al., 2015). Moreover, users adopt retweets quicker than using hashtags in individual tweets (Oliveira et al., 2021). Thus, the analysis focuses on the retweet network of the vaccination hashtag. The following tutorial is based on X (Twitter) data collected during the Covid-19 pandemic and focuses on accounts that retweeted or posted messages using #vaccines during the observation period. Social network analysis enables the visualization and identification of communities. Thereafter, a qualitative inspection of most shared content in each community enables the identification of those that amplify disinformation or conspiracy narratives as a specific form of disinformation. In this tutorial, however, the qualitative inspection is limited because X’s (Twitter) data protection guidelines prohibit the sharing of screennames, and a lot of the accounts have been deleted. Thus, the data only includes pseudo-identifiers, and you may be better served by replicating the analysis and network creation with your self-collected data or research data from archives.  ","version":"Next","tagName":"h3"},{"title":"Tutorial on Social Network Analysis of Twitter Networks in R​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#tutorial-on-social-network-analysis-of-twitter-networks-in-r","content":" R is a free software environment for statistical computing and graphics that is used by researchers from a variety of disciplines. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. You can download the software here and can find a step-by-step tutorial on installing R and R Studio here.  In this case you will use R to create and analyze a network data edge list from X (Twitter) data.  First Tab:  d &lt;- df_vaccines # create variable d$isRetweet d$RT &lt;- gsub(&quot;([A-Za-z]+).*&quot;, &quot;\\\\1&quot;, d$text) d$isRetweet &lt;- ifelse(d$RT == &quot;RT&quot;, TRUE, FALSE) # Split into retweets and original tweets sp = split(d, d$isRetweet) orig = sp[['FALSE']] # Extract the retweets and pull the original author's screenname rt = mutate(sp[['TRUE']], sender = substr(text, 5, regexpr(':', text) - 1)) # Adjust retweets to create an edgelist for network el = as.data.frame(cbind(sender = tolower(rt$sender), receiver = tolower(rt$author_id))) class(el$sender) class(el$receiver) el = count(el, sender, receiver) # create duplicate from edgelist df_el &lt;- el # rename columns for import into Gephi (Gephi requires a &quot;Source&quot; and &quot;Target&quot; column in order to read edgelists of directed networks) names(df_el)[1] &lt;- &quot;Target&quot; names(df_el)[2] &lt;- &quot;Source&quot; df_el$Weight &lt;- df_el$n # save the edgelist in csv-format (comma-spearated values) for easy import to Gephi write.csv(df_el, file = &quot;Vaccines_retweet_network.csv&quot;, row.names = FALSE)   Now you can 1) continue the analysis in R OR 2) import the edgelist into the open-source software Gephi to visualize and further analyze the network.  The most frequently used packages for network analysis in R are network (https://cran.r-project.org/web/packages/network/index.html), igraph (https://cran.r-project.org/web/packages/igraph/index.html), and sna (https://cran.r-project.org/web/packages/sna/index.html).  note A great introductory tutorial on Igraph is available here created by Katherine Ognyanova or here created by the developers of the igraph package.  Start by loading your edgelist to R:  # install packages install.packages('twitteR', 'dplyr', 'ggplot2', 'lubridate', 'network', 'sna', 'igraph', 'network', 'qdap', 'tm') install.packages(&quot;remotes&quot;) remotes::install_github(&quot;analyxcompany/ForceAtlas2&quot;) # Once installed, load the packages using the library() function: library(igraph) library(network) library(sna) library(dplyr) library(utils) library(ForceAtlas2) &quot;Step 2: Importing network data Next, you'll need to import your network data into R. The format of your data will depend on the specific network analysis you want to perform. For example, if you have an edge list (a list of connections between nodes), you can import it using the read.table() function:&quot; # Importing edge list (will be from a GitHub Page; GDrive or Dropbox) edge_list &lt;- read_csv(&quot;~/Documents/Hertie/Freelance/Bertelsmann Stiftung/Knowledge Hub/Bertelsmann Knowledge Hub_SNA Tutorial_Darius/Vaccine_edgelist_rtnet.csv&quot;) edge_list &lt;- Vaccine_edgelist_rtnet # make sure there are no NAs in your edgelist edge_list &lt;- na.omit(edge_list) # write_csv(edge_list, &quot;Vaccine_edgelist_rtnet.csv&quot;) &quot;If you have data in a different format, you may need to use a different function to import it. Step 3: Creating a network object once you have imported your network data, you need to create a network object that R can work with. The specific function you use will depends on the package you are using.&quot; # Create an igraph network object network_igraph &lt;- graph_from_data_frame(d = edge_list, directed = TRUE) # If you are using the network package, you can create a network object from the edge list using the network() function: rtnet &lt;- network(edge_list, directed = TRUE, loops = TRUE, multiple = TRUE) # Step 4: Basic network visualization To visualize the network, you can use the plot() function from the igraph package: # Visualize the network plot(rtnet, remove.multiple = F, remove.loops = T, edge.arrow.size=.4,vertex.label=NA, vertex.size2 = 0.1) plot(network_igraph, remove.multiple = F, remove.loops = T, edge.arrow.size=.4,vertex.label=NA, vertex.size2 = 0.1) # this shows only a 'ball' if nodes but we can see already a clustered structure # Large Graph Layout layout_with_lgl(network_igraph) # try ForceAtlas2 layout algorithm (this is computationally expansive an will take some time due to the size of the network) layout &lt;- layout.forceatlas2(network_igraph, iterations=3000, plotstep=100) plot(network_igraph, layout=layout) &quot;Step 5: Analyzing network properties Now that you have your network object, you can analyze various network properties. Here we focus on degree centrality and betweenness centrality: Degree centrality: The degree centrality of a node is the number of connections it so the number of edges adjacent to it. In case of directed networks, we can also define in-degree (the number of edges pointing towards the node/vertex) and out-degree (the number of edges originating from the node/vertex).&quot; # Network measures with Igraph (alternatively you can also calculate them with other network analysis packages for R like &quot;network&quot;, here we are using IGraph) degree(network_igraph) # Betweenness centrality: The betweenness centrality of a node measures the extent to which it lies on the shortest paths between other nodes. You can calculate the betweenness centrality using the betweenness() function: edge_betweenness(network_igraph) # the network package also allows for easy estimation of further centrality measures such as eigenvector centrality or closeness centrality (see 6.7 Centrality &amp; Centralization in https://kateto.net/netscix2016.html)   If you are new to network analysis, you may want to start by exploring the network graph in Gehpi and then turning to further analysis in R or Python. Gephi works well with large network data sets and produces extremely good-looking graphs providing color palettes, and a variety of layout algorithms and network measures.  ","version":"Next","tagName":"h3"},{"title":"Tutorial on Social Network Analysis of Twitter Networks in Python​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#tutorial-on-social-network-analysis-of-twitter-networks-in-python","content":" This is a short introduction on social network analysis in Python. Python is a high-level, general-purpose programming language used by many researchers and data scientists. In this example you run the script in Google Colab but you can also run the script in Jupyter Notebooks or an integrated development environment (IDE) of your choice.  # Intro2SNA - Analysing Social Networks with Python # Step 1: Install and load the igraph package First, make sure you have the igraph package installed. Run the following code to install it: !pip install python-igraph !pip install pandas !pip install pycairo !pip install cairocffi # Then, load the package using the import statement: import igraph as ig # Step 2: Import the edgelist data Assuming you have an edgelist CSV file with columns 'source' and 'target' representing retweet connections, you can import it using the read() function from the pandas library: import pandas as pd # mount Drive (if in Colabs or load from local disk or GitHub) from google.colab import drive drive.mount('/content/drive') # Import the edgelist data edgelist = pd.read_csv('/content/drive/MyDrive/Vaccine_edgelist_rtnet.csv') # Step 3: Create a network object Next, create a network object using igraph's Graph() function. You can pass the source and target columns from the edgelist DataFrame as arguments: # Create a graph from the edgelist data network = ig.Graph.TupleList(edgelist.itertuples(index=False), directed=True) # Step 4: Basic network visualization To visualize the network, use the plot() function: ig.plot(network, bbox=(2000, 2000)) # This will generate a basic plot of the retweet network. # Step 5: Analyzing network properties Now, you can analyze various network properties using igraph. Here are a few examples: # Degree centrality: Calculate the in-degree centrality (number of retweets received) using the indegree() function: indegree_centrality = network.indegree() print(indegree_centrality) #plt.plot(indegree_centrality) # Betweenness centrality: Calculate the betweenness centrality using the betweenness() function: betweenness_centrality = network.betweenness() print(betweenness_centrality) # Community detection: Detect communities within the network using edge-betweenness for directed graphs: community_EB = network.community_edge_betweenness() print(community_EB) # Igraph and other packages provide many more community detection algorithms that you can compare https://igraph.org/python/doc/api/igraph.Graph.html#community_multilevel # Step 6: To visualize network properties, you can use different plotting functions. Here are a few examples: # Visualizing in-degree centrality ig.plot(network, vertex_size=indegree_centrality, bbox=(4000, 4000)) # Visualizing betweenness centrality: ig.plot(network, vertex_size=betweenness_centrality, bbox=(4000, 4000)) # As we can see the network graphs look rather messy but by adjusting the layout, node size and gravity we can make them look nicer. In Gephi, however, the visualization of large networks is much more straight-forward.   You can find more extensive tutorials for further investigation of networks in Python on YouTube, GitHub or by searching the Web. These are well done examples:  Network Analysis and Community Detection on Political Tweets by Kristina Popova on MediumIGraph TutorialNetworkx Tutorial  ","version":"Next","tagName":"h2"},{"title":"Implementing Social Network Analysis of X (Twitter) Networks in Gephi​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#implementing-social-network-analysis-of-x-twitter-networks-in-gephi","content":" Gephi is an open-source software tool for (social) network analysis and alllows for great-looking visualizations, even of large-scale networks. You can download Gephi for free here.  Now import our edgelist file Vaccine_edgelist_rtnet.csv in Gephi as a spreadsheet: File &gt; Import spreadsheet” and select “Separator = Comma” (because we are importing a comma-separated values file), “Import as: Edges table” and “Charset = UTF-8”, which should be the default anyway. In the second import step you need to select a time representation, which is irrelevant in this case but matters if you are importing dynamic network data that inspects changes over time. You can select “Time representation = Intervals” and press “Finish”.  Now the import report window should open and show you 4593 nodes and 8554 edges that were correctly read (you canignore all values with missing source or target nodes IDs). Choose “Graph Type = Directed”, because retweeting someone has a direction and we are interested in who was most retweeted by others and who most frequently retweeted others to detect patterns in accounts’ behavior. Select “New workspace”, press “More options…” and “Edges merge strategy = SUM”, then press “OK” at the bottom right corner.  Now the imported network data appears as a black square on your Gephi window (see Screenshot 1 below).    On the right-hand side (red circle) of the Gephi interface you can run analyses by pressing the “Run” button. For this tutorial press “RUN” for “Average Degree”, “Avg. Weighted Degree” and “Modularity” as a community detection method based on the Louvain algorithm. In the Box that opens click “Randomize” and “Use Weights”, then choose “Resolution: 5.0”. Increasing the resolution parameter, based on Lambiotte et al. (2009), results in less-fractioned communities, if they seem too large for you, you can re-run the modularity algorithm with a lower resolution parameter.  On the left-hand side (blue circle) of the Gephi interface, you can choose a layout algorithm to visualize the network. Select ForceAtlas2 and press “Run” with the default parameters. Now the black square starts to disintegrate as the layout algorithms force unconnected nodes to repel each other. Let the network expand until it appears to not change anymore and converge.  Now you can already see several distinct clusters where nodes are more densely connected with each other than with nodes in other clusters. The next step assigns a color to network nodes based on the modularity groups that were identified with the Louvain algorithm.  In screenshot 2 the green circle indicates where to select “Partition” by “Modularity Class” in the top left corner to assign modularity class color to nodes in the network. You can change the colors by clicking on the color squares and choosing from a color palette for each community or choose a pre-saved palette by clicking on “Palette” in the bottom right corner. Choose colors or a palette with sufficient contrast to the background to help with accessibility.    After applying the modularity class-based coloring you can see that that community membership largely collides with the network clusters. Now you use qualitative content analysis on a sample of accounts or the most retweeted account (those with the highest indegree) to get an idea of the sort of content that is shared in each of the identified communities. To do so, click on the top bar on “Data Laboratory” and sort for weighted indegree, which is the sum of times an account was retweeted. Now you can inspect the accounts that were most retweeted, their position in the network graph and look them up on X (Twitter). For this tutorial we have dehydrated or anonymized for privacy protection and to comply with X’s (Twitter’s) data sharing regulations. Within your self-collected data, you can look up accounts via usernames or X (Twitter)-IDs.  The following video briefly illustrates the steps of analysis in Gephi:    note On YouTube and other platforms there are many more great Gephi video tutorials. For instance, I would recommend Martin Grandjean's tutorial for the start, which you can find here.  ","version":"Next","tagName":"h2"},{"title":"Research Example​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#research-example","content":" In Darius and Urquhart (2021) we inspect the emergence and growth of conspiracy communities on UK Twitter during the Covid-19 pandemic using social network analyis and qualitative content analysis with which we identify those communities and accounts that issue and share messages aligned with conspiracy narratives about vaccination (e.g. that vaccinations implant micro-chips to control the population). In general the detection of communities with community detection algrorithms should be accompanied with automated text analysis or qualitative analysis of content circulated in the identified communities. In the study we identified a high presence of tweets referencing conspiracy narratives and noted an overproportional growth of conspiracy communities. The following figure from Darius and Urquhart (2021) illustraes the anti-vaccination community before and after the introduction of so-called lockdown policies to reduce the spread of Covid-19. The graph illustrate that the anti-vaccination movement constitues about a third of retweets with regard to the #Vaccination during the later observation period.    This resonated with studies by others, that identifed a growing presence of conspiracy narratives about vaccinations but also with respect to 5G during the Covid19 pandemic (Ahmed et al., 2020; Yang et al., 2021). You can find a lot of interdisciplinary research using network approaches in the journals “Social Networks”, “Online Social Networks and Mining”, or “Online Social Networks and Media”. Have fun exploring the world of social and complex networks!  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Introduction to social network analysis in R and Gephi","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/social-network-analysis#references","content":" Ahmed, Wasim, Josep Vidal-Alaball, Joseph Downing, and Francesc López Seguí. “COVID-19 and the 5G Conspiracy Theory: Social Network Analysis of Twitter Data.” Journal of Medical Internet Research 22, no. 5 (May 6, 2020): e19458. https://doi.org/10.2196/19458.  Blondel, Vincent D., Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. “Fast Unfolding of Communities in Large Networks.” Journal of Statistical Mechanics: Theory and Experiment 2008, no. 10 (October 9, 2008): P10008. https://doi.org/10.1088/1742-5468/2008/10/P10008.  Boyd, Danah, Scott Golder, and Gilad Lotan. “Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter.” In 2010 43rd Hawaii International Conference on System Sciences, 1–10, 2010. https://doi.org/10.1109/HICSS.2010.412.  Bruns, Axel, Brenda Moon, Avijit Paul, and Felix Münch. “Towards a Typology of Hashtag Publics: A Large-Scale Comparative Study of User Engagement across Trending Topics.” Communication Research and Practice 2, no. 1 (January 2, 2016): 20–46. https://doi.org/10.1080/22041451.2016.1155328.  Caplan, Robyn, and Danah Boyd. “Who Controls the Public Sphere in an Era of Algorithms.” Mediation, Automation, Power, 2016, 1–19.  Carrington, Peter J., John Scott, and Stanley Wasserman. Models and Methods in Social Network Analysis. Vol. 28. Cambridge university press, 2005.  Conover, Michael D., Bruno Gonçalves, Alessandro Flammini, and Filippo Menczer. “Partisan Asymmetries in Online Political Activity.” EPJ Data Science 1, no. 1 (December 2012): 1–19. https://doi.org/10.1140/epjds6.  Darius, Philipp, and Michael Urquhart. “Disinformed Social Movements: A Large-Scale Mapping of Conspiracy Narratives as Online Harms during the COVID-19 Pandemic.” Online Social Networks and Media 26 (November 2021): 100174. https://doi.org/10.1016/j.osnem.2021.100174.  Gaisbauer, Felix, Armin Pournaki, Sven Banisch, and Eckehard Olbrich. “Grounding Force-Directed Network Layouts with Latent Space Models.” Journal of Computational Social Science, May 29, 2023. https://doi.org/10.1007/s42001-023-00207-w.  Lambiotte, R., J.-C. Delvenne, and M. Barahona. “Laplacian Dynamics and Multiscale Modular Structure in Networks.” IEEE Transactions on Network Science and Engineering 1, no. 2 (July 1, 2014): 76–90. https://doi.org/10.1109/TNSE.2015.2391998.  Lancichinetti, Andrea, and Santo Fortunato. “Community Detection Algorithms: A Comparative Analysis.” Physical Review E 80, no. 5 (November 30, 2009): 056117. https://doi.org/10.1103/PhysRevE.80.056117.  Metaxas, Panagiotis, Eni Mustafaraj, Kily Wong, Laura Zeng, Megan O’Keefe, and Samantha Finn. “What Do Retweets Indicate? Results from User Survey and Meta-Review of Research.” Proceedings of the International AAAI Conference on Web and Social Media 9, no. 1 (2015): 658–61.  Newman, M. E. J. “Modularity and Community Structure in Networks.” Proceedings of the National Academy of Sciences 103, no. 23 (June 6, 2006): 8577–82. https://doi.org/10.1073/pnas.0601602103.  Oliveira, Jaqueline Faria de, Humberto Torres Marques-Neto, and Márton Karsai. “Measuring the Effects of Repeated and Diversified Influence Mechanism for Information Adoption on Twitter.” Social Network Analysis and Mining 12, no. 1 (December 8, 2021): 16. https://doi.org/10.1007/s13278-021-00844-x.  Schoch, David, Franziska B. Keller, Sebastian Stier, and JungHwan Yang. “Coordination Patterns Reveal Online Political Astroturfing across the World.” Scientific Reports 12, no. 1 (March 17, 2022): 4572. https://doi.org/10.1038/s41598-022-08404-9.  Scott, John. Social Network Analysis. 1 Oliver’s Yard, 55 City Road London EC1Y 1SP: SAGE Publications Ltd, 2017. https://doi.org/10.4135/9781529716597.  Yang, Kai-Cheng, Francesco Pierri, Pik-Mai Hui, David Axelrod, Christopher Torres-Lugo, John Bryden, and Filippo Menczer. “The COVID-19 Infodemic: Twitter versus Facebook.” Big Data &amp; Society 8, no. 1 (January 2021): 205395172110138. https://doi.org/10.1177/20539517211013861.  Yang, Song, Franziska B. Keller, and Lu Zheng. Social Network Analysis: Methods and Examples. SAGE Publications, 2016. ","version":"Next","tagName":"h2"},{"title":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules","content":"","keywords":"","version":"Next"},{"title":"Best Practices for Creating Queries and Rules​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#best-practices-for-creating-queries-and-rules","content":" Here are some tips and best practices for creating effective rules for querying the X (Twitter) API:  Define clear goals: Before you start querying, know what information you need. Do you want to collect tweets about a specific topic, from a specific location, or at a specific time? The clearer your goals, the more efficient your query will be. Keyword research: Take the time to research relevant keywords, phrases, and hashtags. This will help you focus your data request. Use Boolean Operators: You can use Boolean operators such as AND, OR, and NOT to make your search criteria more specific. For example, climate change AND research returns tweets that contain both terms. Consider alternate spellings: When focusing on keywords or hashtags, consider alternate spellings, abbreviations, or misspellings. Use exclusions: To filter unwanted results from your query, you can exclude certain words or phrases. Use geolocation features: If your research interest is geographically limited, use the X’s (Twitter’s) API geolocation feature to collect tweets from specific regions. (Note: Only a small number of tweets contain geo-information.) Set time limits: If relevant, you can limit your query to a specific time period. Test and tweak: Start with a broad query and narrow it down incrementally. Review the results and adjust your query as needed. Consider usage limits: The X (Twitter) API has usage limits. Make sure you understand them and plan your queries accordingly.  ","version":"Next","tagName":"h2"},{"title":"Building a query or rule​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#building-a-query-or-rule","content":" If you are using the Search Tweets endpoint (Recent Search and Full Search), the filter is called a query. And if you are using the Filtered Stream endpoint, it is called a rule.  ","version":"Next","tagName":"h2"},{"title":"Query limitations​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#query-limitations","content":" Depending on your access level, the length of queries or the number of rules may be limited.  ","version":"Next","tagName":"h3"},{"title":"Operator types: standalone and conjunction-required​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#operator-types-standalone-and-conjunction-required","content":" Operators are distinguished in two types: standalone operators and conjunction-required operators. Standalone operators need not necessarily be used in conjunction with other operators, but they can be.  The following query utilizes the #hashtag operator, which is a standalone operator.  #science   Conjunction-required operators presuppose the use of at least one standalone operator in the query. Otherwise, they would be too broad in scope and the query would generate an excessive number of Tweets. The following examples do not contain standalone operators and thus are not legitimate queries.  has:mentions has:media OR is:verified   If we add in a standalone operator, such as the phrase “twitter data”, the query will work properly. The above example can be made valid by adding a standalone operator. For example:  &quot;research results&quot; has:mentions (has:media OR has:links)   ","version":"Next","tagName":"h3"},{"title":"Boolean operators and grouping​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#boolean-operators-and-grouping","content":" Multiple keywords can be combined by using boolean operators. Those are short words (e.g. AND, OR) that can be used to either expand the scope of the query or to specify the query.  \tAND logic\tQueries containing keywords combined by the boolean AND logic will yield only Tweets containing all the keywords. Spaces between keywords are implicitly interpreted as AND logic. For example, research results #ScientificBreakthrough will only match Tweets containing the words research and results as well as the hashtag #ScientificBreakthrough. OR logic\tCombining keywords with the OR-operator expands the scope of the query. It will find every tweet containing at least one of the given keywords. For example, human OR resources OR #meme will retrieve all tweets that include at least one of the terms human, resources or the hashtag #meme. NOT logic, negation\tTo apply negation (NOT) in logic, add a hyphen (-) before a keyword or operator. For instance, in the query science #meme -informatics, the search will identify posts with both #meme and science, excluding those that also have the term informatics. A frequently used example is -is:retweet, excluding Retweets and allowing matches for original Tweets, Quote Tweets, and replies. While all operators can be negated, standalone negated operators are not functional. Grouping\tGrouping operators is possible with parentheses. For instance, (research results) OR (#meme has:images) will yield Tweets that have either the research and results terms, or images tagged with the #meme hashtag. The sequence of evaluation involves ANDs first, then ORs.  When using both AND and OR functionalities together, the order of operations is as follows:  Operators that are linked by AND logic are combined as the first step.Afterward, operators connected through OR logic are applied.  For example:  dog OR cat mouse will be interpreted as dog OR (cat mouse)horse cow OR sheep will be interpreted as (horse cow) OR sheep  To remove any vagueness and ensure accurate evaluation of your rule, use parentheses to group terms together when necessary.  For example:  (dog OR cat) mousehorse (cow OR sheep)  ","version":"Next","tagName":"h2"},{"title":"Punctuation, diacritics, and case sensitivity​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#punctuation-diacritics-and-case-sensitivity","content":" Characters that include accents or diacritics are handled just like regular characters, without being regarded as word separators. For instance, a rule with the keyword jalapeños would solely identify Tweets containing the exact term jalapeños, without considering matches like jalape, jalapen, or os.  All operators are treated case-insensitive. For example, the query science will provide the same results as science, SCIENCE, Science.  Rules containing accents or diacritics lead to distinct behaviors between the Filtered Stream and Search endpoints.  ","version":"Next","tagName":"h2"},{"title":"Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#filtered-stream","content":" When you define a keyword or hashtag rule that includes character accents or diacritics, it will identify Tweets containing the precise word with the correct accents or diacritics. It won’t include Tweets with accurate letters but lacking accents or diacritic marks.  For example, rules with the keyword Résumé or hashtag #jalapeños will match Tweets that contain Résumé or #jalapeños because they include the accents or diacritic. These rules will not match Tweets that contain Resume or #jalapenos.  ","version":"Next","tagName":"h3"},{"title":"Search Tweets​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#search-tweets","content":" Unlike Filtered Stream endpoints, Search endpoints are not insensitive to accents and diacritics. This means that queries containing them will return both terms with and without accents or diacritics. For example, the querys Résumé or hashtag #jalapeños will yield Tweets containing Résumé or #jalapeños as well as those containing Resume or #jalapenos.  For example, the queries Résumé or hashtag #jalapeños will yield Tweets containing Résumé or #jalapeños as well as those containing Resume or #jalapenos.  ","version":"Next","tagName":"h3"},{"title":"Quote Tweet matching behavior​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#quote-tweet-matching-behavior","content":" ","version":"Next","tagName":"h2"},{"title":"Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#filtered-stream-1","content":" Operators will apply to both the content present in the initial Tweet that was quoted and the content present within the Quote Tweet.  ","version":"Next","tagName":"h3"},{"title":"Search Tweets​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#search-tweets-1","content":" Operators will not find matches in the content of the original Tweet that was quoted, but they will match content present in the Quote Tweet.  ","version":"Next","tagName":"h3"},{"title":"Iteratively building a rule​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#iteratively-building-a-rule","content":" ","version":"Next","tagName":"h2"},{"title":"Test your rule early and often​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#test-your-rule-early-and-often","content":" Achieving accurate results with a rule on the first attempt is uncommon. Due to the sheer volume and variety of Tweets it is rarely evident which exact Tweets the search will return.  In the process of creating a rule, it is therefore essential to frequently test it using the stream endpoint to observe the data it retrieves. You should also consider testing it using one of the Search Tweet endpoints, provided that the operators you use are also compatible with that endpoint.  In the following we will start with this simple rule and develop it in dependence on the output it generates:  create OR creation   ","version":"Next","tagName":"h3"},{"title":"Use results to narrow the rule​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#use-results-to-narrow-the-rule","content":" While testing the rule, it is essential to review the returned Tweets to verify if they contain the anticipated and desired data. It is recommended to start with a broad rule that typically generates a large set of Tweets. Afterwards this rule can be refined to exclude unwanted results.  Since with the existing rule we have obtained tweets in multiple languages the following specification limits the results to Tweets in English.  (create OR creation) lang:en   The test resulted in several Tweets praising divine creation. We are going to remove them from the results by adding the negated keyword operator -divine. Furthermore, we do not want to include retweets. We can reach that goal by adding the negated -is:retweet operator.  (create OR creation) lang:en -divine -is:retweet   ","version":"Next","tagName":"h3"},{"title":"Adjust for inclusion where needed​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#adjust-for-inclusion-where-needed","content":" If the query does not return certain Tweets that you know exist, you might need to widen your rule by eliminating operators that could potentially lead to the exclusion of the desired data.  We noticed that there are Tweets treating the same topic that are not included in our search results. That is because they use different terms similar or equal meaning. To cover those Tweets, we can add those terms to the rule:  (create OR creation OR making OR founding) lang:en -divine -is:retweet   ","version":"Next","tagName":"h3"},{"title":"Adjust for popular trends/bursts over the time period​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#adjust-for-popular-trendsbursts-over-the-time-period","content":" Since X (Twitter) is a highly dynamic platform, your rules may need to be adapted to upcoming and outdated trends. We therefore recommend updating and adjusting your rules periodically.  ","version":"Next","tagName":"h3"},{"title":"Operators​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#operators","content":" Essential: Available with all access levels.Elevated: Available when using a Project with Elevated, Academic Research, or Enterprise access.Certain operators have an alternate name or alias that can be used.  ","version":"Next","tagName":"h2"},{"title":"Available for Search Tweets and Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#available-for-search-tweets-and-filtered-stream","content":" Operator\tType\tAvailability\tDescriptionkeyword\tStandalone\tEssential\tMatches a keyword within the body of a Tweet. This is a tokenized match, meaning that your keyword string will be matched against the tokenized text of the Tweet body. Tokenization splits words based on punctuation, symbols, and Unicode basic plane separator characters. For example, a Tweet with the text “I like coca-cola” would be split into the following tokens: I, like, coca, cola. These tokens would then be compared to the keyword string used in your query. To match strings containing punctuation (for example coca-cola), symbol, or separator characters, you must wrap your keyword in double-quotes. Example: pepsi OR cola OR &quot;coca cola&quot; emoji\tStandalone\tEssential\tMatches an emoji within the body of a Tweet. Similar to a keyword, emojis are a tokenized match, meaning that your emoji will be matched against the tokenized text of the Tweet body. Note that if an emoji has a variant, you must wrap it in double quotes to add to a query. Example: (😃 OR 😡) 😬 &quot;exact phrase match&quot;\tStandalone\tEssential\tMatches the exact phrase within the body of a Tweet. Example: (&quot;Twitter API&quot; OR #v2) -&quot;recent search&quot; #\tStandalone\tEssential\tMatches any Tweet containing a recognized hashtag, if the hashtag is a recognized entity in a Tweet. This operator performs an exact match, NOT a tokenized match, meaning the rule #thanku will match posts with the exact hashtag #thanku, but not those with the hashtag #thankunext. Example: #thankunext #fanart OR @arianagrande @\tStandalone\tEssential\tMatches any Tweet that mentions the given username, if the username is a recognized entity (including the @ character). Example: (@twitterdev OR @twitterapi) -@twitter $\tStandalone\tElevated\tMatches any Tweet that contains the specified ‘cashtag’ (where the leading character of the token is the ‘$’ character). Note that the cashtag operator relies on Twitter’s ‘symbols’ entity extraction to match cashtags, rather than trying to extract the cashtag from the body itself. Example: $twtr OR @twitterdev -$fb from:\tStandalone\tEssential\tMatches any Tweet from a specific user. The value can be either the username (excluding the @ character) or the user’s numeric user ID. You can only pass a single username/ID per from: operator. Example: from:twitterdev OR from:twitterapi -from:twitter to:\tStandalone\tEssential\tMatches any Tweet that is in reply to a particular user. The value can be either the username (excluding the @ character) or the user’s numeric user ID. You can only pass a single username/ID per to: operator. Example: to:twitterdev OR to:twitterapi -to:twitter url:\tStandalone\tEssential\tPerforms a tokenized match on any validly-formatted URL of a Tweet. This operator can matches on the contents of both the url or expanded_url fields. For example, a Tweet containing &quot;You should check out Twitter Developer Labs: https://t.co/c0A36SWil4&quot; (with the short URL redirecting to https://developer.twitter.com) will match both the following rules: from:TwitterDev url:&quot;https://developer.twitter.com&quot; (because it will match the contents of entities.urls.expanded_url) from:TwitterDev url:&quot;https://t.co&quot; (because it will match the contents of entities.urls.url) Tokens and phrases containing punctuation or special characters should be double-quoted (for example, url:&quot;/developer&quot;). Similarly, to match on a specific protocol, enclose in double-quotes (for example, url:&quot;https://developer.twitter.com&quot;). retweets_of:\tStandalone\tEssential\tMatches Tweets that are Retweets of the specified user. The value can be either the username (excluding the @ character) or the user’s numeric user ID. You can only pass a single username/ID per retweets_of: operator. Example: retweets_of:twitterdev OR retweets_of:twitterapi in_reply_to_tweet_id:\tStandalone\tEssential\tAvailable alias: in_reply_to_status_id: Matches on replies to the specified Tweet. Example: in_reply_to_tweet_id:1539382664746020864 retweets_of_tweet_id:\tStandalone\tEssential\tAvailable alias: retweets_of_status_id: Matches on explicit (or native) Retweets of the specified Tweet. Note that the Tweet ID used should be the ID of an original Tweet and not a Retweet. Example: retweets_of_tweet_id:1539382664746020864 quotes_of_tweet_id:\tStandalone\tEssential\tAvailable alias: quotes_of_status_id: Matches on Quote Tweets of the specified Tweet. Note that the Tweet ID used should be the ID of an original Tweet and not a Quote Tweet. Example: quotes_of_tweet_id:1539382664746020864 context:\tStandalone\tEssential\tMatches Tweets with a specific domain id/entity id pair. To learn more about this operator, please visit our page on annotations. You can only pass a single domain/entity per context: operator. context:domain_id.entity_id However, you can combine multiple domain/entities using the OR operator: (context:47.1139229372198469633 OR context:11.1088514520308342784) Examples: context:10.799022225751871488 (domain_id.entity_id returns Tweets matching that specific domain-entity pair) entity:\tStandalone\tEssential\tMatches Tweets with a specific entity string value. To learn more about this operator, please visit our page on annotations. Please note that this is only available with recent search. You can only pass a single entity: operator. entity:&quot;string declaration of entity/place&quot; Examples: entity:&quot;Michael Jordan&quot; OR entity:&quot;Barcelona&quot; conversation_id:\tStandalone\tEssential\tMatches Tweets that share a common conversation ID. A conversation ID is set to the Tweet ID of a Tweet that started a conversation. As Replies to a Tweet are posted, even Replies to Replies, the conversation_id is added to its JSON payload. You can only pass a single conversation ID per conversation_id: operator. Example: conversation_id:1334987486343299072 (from:twitterdev OR from:twitterapi) list:\tStandalone\tElevated\tNEW Matches Tweets posted by users who are members of a specified list. For example, if @twitterdev and @twitterapi were members of List 123, and you included list:123 in your query, your response will only contain Tweets that have been published by those accounts. You can find List IDs by using the List lookup endpoint. Please note that you can only use a single list: operator per query, and you can only specify a single List per list: operator. Example: list:123 place:\tStandalone\tElevated\tMatches Tweets tagged with the specified location or Twitter place ID. Multi-word place names (“New York City”, “Palo Alto”) should be enclosed in quotes. You can only pass a single place per place: operator. Note: See the GET geo/search standard v1.1 endpoint for how to obtain Twitter place IDs. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: place:&quot;new york city&quot; OR place:seattle OR place:fd70c22040963ac7 place_country:\tStandalone\tElevated\tMatches Tweets where the country code associated with a tagged place/location matches the given ISO alpha-2 character code. You can find a list of valid ISO codes on Wikipedia. You can only pass a single ISO code per place_country: operator. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: place_country:US OR place_country:MX OR place_country:CA point_radius:\tStandalone\tElevated\tMatches against the place.geo.coordinates object of the Tweet when present, and in X (Twitter), against a place geo polygon, where the Place polygon is fully contained within the defined region. point_radius:[longitude latitude radius] Units of radius supported are miles (mi) and kilometers (km) Radius must be less than 25mi Longitude is in the range of ±180 Latitude is in the range of ±90 All coordinates are in decimal degrees Rule arguments are contained within brackets, space delimited You can only pass a single geo polygon per point_radius: operator. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: point_radius:[2.355128 48.861118 16km] OR point_radius:[-41.287336 174.761070 20mi]&lt;br /&gt; bounding_box:\tStandalone\tElevated\tAvailable alias: geo_bounding_box: Matches against the place.geo.coordinates object of the Tweet when present, and in X (Twitter), against a place geo polygon, where the place polygon is fully contained within the defined region. bounding_box:[west_long south_lat east_long north_lat] west_long south_lat represent the southwest corner of the bounding box where west_long is the longitude of that point, and south_lat is the latitude. east_long north_lat represent the northeast corner of the bounding box, where east_long is the longitude of that point, and north_lat is the latitude. Width and height of the bounding box must be less than 25mi Longitude is in the range of ±180 Latitude is in the range of ±90 All coordinates are in decimal degrees. Rule arguments are contained within brackets, space delimited. You can only pass a single geo polygons per bounding_box: operator. Note: This operator will not match on Retweets, since Retweet's places are attached to the original Tweet. It will also not match on places attached to the original Tweet of a Quote Tweet. Example: bounding_box:[-105.301758 39.964069 -105.178505 40.09455]&lt;br /&gt; is:retweet\tConjunction required\tEssential\tMatches on Retweets that match the rest of the specified rule. This operator looks only for true Retweets (for example, those generated using the Retweet button). Quote Tweets will not be matched by this operator. Example: data @twitterdev -is:retweet is:reply\tConjunction required\tEssential\tDeliver only explicit replies that match a rule. Can also be negated to exclude replies that match a query from delivery. Note: This operator is also available with the filtered stream endpoint. When used with filtered stream, this operator matches on replies to an original Tweet, replies in quoted Tweets, and replies in Retweets. Example: from:twitterdev is:reply is:quote\tConjunction required\tEssential\tReturns all Quote Tweets, also known as Tweets with comments. Example: &quot;sentiment analysis&quot; is:quote is:verified\tConjunction required\tEssential\tDeliver only Tweets whose authors are verified by Twitter. Example: #nowplaying is:verified -is:nullcast\tConjunction required\tElevated\tRemoves Tweets created for promotion only on ads.twitter.com that have a &quot;source&quot;:&quot;Twitter for Advertisers (legacy)&quot; or &quot;source&quot;:&quot;Twitter for Advertisers&quot;. This operator must be negated. For more info on Nullcasted Tweets, see our page on Tweet availability. Example: &quot;mobile games&quot; -is:nullcast has:hashtags\tConjunction required\tEssential\tMatches Tweets that contain at least one hashtag. Example: from:twitterdev -has:hashtags has:cashtags\tConjunction required\tElevated\tMatches Tweets that contain a cashtag symbol (with a leading ‘$’ character. For example, $tag). Example: #stonks has:cashtags has:links\tConjunction required\tEssential\tThis operator matches Tweets which contain links and media in the Tweet body. Example: from:twitterdev announcement has:links has:mentions\tConjunction required\tEssential\tMatches Tweets that mention another Twitter user. Example: #nowplaying has:mentions has:media\tConjunction required\tEssential\tAvailable alias: has:media_link Matches Tweets that contain a media object, such as a photo, GIF, or video, as determined by Twitter. This will not match on media created with Periscope, or Tweets with links to other media hosting sites. Example: (kittens OR puppies) has:media has:images\tConjunction required\tEssential\tMatches Tweets that contain a recognized URL to an image. Example: #meme has:images has:video_link\tConjunction required\tEssential\tAvailable alias: has:videos Matches Tweets that contain native Twitter videos, uploaded directly to Twitter. This will not match on videos created with Periscope, or Tweets with links to other video hosting sites. Example: #icebucketchallenge has:video_link has:geo\tConjunction required\tElevated\tMatches Tweets that have Tweet-specific geolocation data provided by the Twitter user. This can be either a location in the form of a Twitter place, with the corresponding display name, geo polygon, and other fields, or in rare cases, a geo lat-long coordinate. Note: Operators matching on place (Tweet geo) will only include matches from original tweets. Retweets do not contain any place data. Example: recommend #paris has:geo -bakery lang:\tConjunction required\tEssential\tMatches Tweets that have been classified by Twitter as being of a particular language (if, and only if, the tweet has been classified). It is important to note that each Tweet is currently only classified as being of one language, so AND’ing together multiple languages will yield no results. You can only pass a single BCP 47 language identifier per lang: operator. Note: if no language classification can be made the provided result is ‘und’ (for undefined). Example: recommend #paris lang:en The list below represents the currently supported languages and their corresponding BCP 47 language identifier: Amharic: am German: de Malayalam: ml Slovak: sk Arabic: ar Greek: el Maldivian: dv Slovenian: sl Armenian: hy Gujarati: gu Marathi: mr Sorani Kurdish: ckb Basque: eu Haitian Creole: ht Nepali: ne Spanish: es Bengali: bn Hebrew: iw Norwegian: no Swedish: sv Bosnian: bs Hindi: hi Oriya: or Tagalog: tl Bulgarian: bg Latinized Hindi: hi-Latn Panjabi: pa Tamil: ta Burmese: my Hungarian: hu Pashto: ps Telugu: te Croatian: hr Icelandic: is Persian: fa Thai: th Catalan: ca Indonesian: in Polish: pl Tibetan: bo Czech: cs Italian: it Portuguese: pt Traditional Chinese: zh-TW Danish: da Japanese: ja Romanian: ro Turkish: tr Dutch: nl Kannada: kn Russian: ru Ukrainian: uk English: en Khmer: km Serbian: sr Urdu: ur Estonian: et Korean: ko Simplified Chinese: zh-CN Uyghur: ug Finnish: fi Lao: lo Sindhi: sd Vietnamese: vi French: fr Latvian: lv Sinhala: si Welsh: cy Georgian: ka Lithuanian: lt   ","version":"Next","tagName":"h3"},{"title":"Only available for Filtered Stream​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#only-available-for-filtered-stream","content":" Operator\tType\tAvailability\tDescriptionfollowers_count: Essential\tMatches Tweets when the author has a followers count within the given range. If a single number is specified, any number equal to or higher will match. Example: followers_count:500 Additionally, a range can be specified to match any number in the given range. Example: followers_count:1000..10000 tweets_count: Essential\tAvailable alias: statuses_count: Matches Tweets when the author has posted a number of Tweets that falls within the given range. If a single number is specified, any number equal to or higher will match. Example: tweets_count:1000 Additionally, a range can be specified to match any number in the given range. Example: tweets_count:1000..10000 following_count: Essential\tAvailable alias: friends_count: Matches Tweets when the author has a friends count (the number of users they follow) that falls within the given range. If a single number is specified, any number equal to or higher will match. Example: following_count:500 Additionally, a range can be specified to match any number in the given range. Example: following_count:1000..10000 listed_count: Essential\tAvailable alias: user_in_lists_count: Matches Tweets when the author is included in the specified number of Lists. If a single number is specified, any number equal to or higher will match. Example: listed_count:10 Additionally, a range can be specified to match any number in the given range. Example: listed_count:10..100 url_title: Essential\tAvailable alias: within_url_title: Performs a keyword/phrase match on the expanded URL HTML title metadata. Example: url_title:snow url_description: Essential\tAvailable alias: within_url_description: Performs a keyword/phrase match on the expanded page description metadata. Example: url_description:weather url_contains: Essential\tMatches Tweets with URLs that literally contain the given phrase or keyword. To search for patterns with punctuation in them (i.e. google.com) enclose the search term in quotes. NOTE: This will match against the expanded URL as well. Example: url_contains:photos source: Essential\tMatches any Tweet generated by the given source application. The value must be either the name of the application or the application’s URL. Cannot be used alone. Example: source:&quot;Twitter for iPhone&quot; Note: As a Twitter app developer, Tweets created programmatically by your application will have the source of your application Name and Website URL set in your app settings. in_reply_to_tweet_id: Essential\tAvailable alias: in_reply_to_status_id: Deliver only explicit Replies to the specified Tweet. Example: in_reply_to_tweet_id:1539382664746020864 retweets_of_tweet_id: Essential\tAvailable alias: retweets_of_status_id: Deliver only explicit (or native) Retweets of the specified Tweet. Note that the status ID used should be the ID of an original Tweet and not a Retweet. Example: retweets_of_tweet_id:1539382664746020864  ","version":"Next","tagName":"h3"},{"title":"Effects of operators​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#effects-of-operators","content":" This table provides examples of how different operators behave when tweets are retweeted, replied to, or users are mentioned.  \tTweet from user1\tRetweet from user1 of a tweet from user2\tRetweet from user2 of a tweet from user1\tReply from user1 to a tweet from user2\tReply from user2 to a tweet from user1\tReply from user3 to a reply of user2 to a tweet of user1\tReply from user3 to a reply from user1 to a tweet of user2\tRetweet of user3 to the reply of user2 to the tweet of user1\tMention of user1 in a tweet of user2\tReply from user 3 to a tweet of user2 where user1 is mentioned\tRetweets of a tweet where user1 is mentionedOriginal tweet author\tuser1\tuser2\tuser1\tuser2\tuser1\tuser1\tuser2\tuser1\tuser2\tuser2\tuser2 conversation_id\tuser1 tweet\tuser2 tweet\tuser2 retweet\tuser2 tweet\tuser1 tweet\tuser1 tweet\tuser2 tweet\tuser1 tweet\tuser2 tweet\tuser2 tweet\tuser3 retweet from:user1\t✅\t✅\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌ from:user1 -is:retweet\t✅\t❌\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌ from:user1 -is:reply\t✅\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌\t❌\t❌ from:user1 -is:quote\t✅\t✅\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌ to:user1\t❌\t❌\t❌\t❌\t✅\t❌\t✅\t✅\t❌\t❌\t❌ @user1\t❌\t❌\t✅\t❌\t✅\t✅\t✅\t✅\t✅\t✅\t✅ @user1 -is:retweet\t❌\t❌\t❌\t❌\t✅\t✅\t✅\t❌\t✅\t✅\t❌ retweets_of:user1\t❌\t❌\t✅\t❌\t❌\t❌\t❌\t❌\t❌\t❌\t❌ midterms user1 is candidate\t✅\t❌\t❌\t✅\t✅\t✅\t✅\t❌\t✅\t✅\t❌  ","version":"Next","tagName":"h3"},{"title":"Sources​","type":1,"pageTitle":"Guidance to Get the Data you Want: How to create apropriate Filter Rules and Queries","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/twitter/twitter-rules#sources","content":" https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rulehttps://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-queryhttps://twitterdev.github.io/do_more_with_twitter_data/finding_the_right_data.htmlhttps://developer.twitter.com/en/docs/tutorials/building-high-quality-filters ","version":"Next","tagName":"h2"},{"title":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis","content":"","keywords":"","version":"Next"},{"title":"Scraping Data on TikTok with puppeteer​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#scraping-data-on-tiktok-with-puppeteer","content":" For this analysis we will leverage the web version of TikTok accessible with any browser on TikTok.com.  While there are various “unofficial APIs” or libraries available that allow you to scrape content from TikTok, the purpose of this chapter is to build a scraper from scratch.  Below, you will find a basic scraping built on pupeteer to open the web version of the FYP.  ","version":"Next","tagName":"h2"},{"title":"Tech Setup​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#tech-setup","content":" To run this project, you should be able to open a command line on your computer and install software. For the first part you need to have nodejs installed. After you have created a folder and navigated to it with the console run npm install pupeteer in it.  A basic scraper that opens the TikTok website should look like this:  const puppeteer = require(&quot;puppeteer&quot;); async function run() { // Setting up the browser const browser = await puppeteer.launch({ executablePath: &quot;/usr/bin/google-chrome&quot;, //we want to run chrome. You need to adjust this depening on your operating system headless: false, // we want to see what pupeteer is doing, headless:true would mean the browser is not shown defaultViewport: null, args: [&quot;--window-size=1920,1080&quot;], }); const page = await browser.newPage(); // opening a new tab await page.goto(&quot;https://tiktok.com/foryou&quot;); // open TikTok } run();   This should bring up a browser windows like this:    This basic process, already allows you to garner information about the videos TikTok would like to show you- even though you can’t see them. You can right-click on the page and select “view source” (or press ctrl+u) to see the HTML source code of the site. Search for “ItemModule” and you will be able to access information about the videos in a structured JSON-format.  The JSON object contains information about the first eight videos that are available or displayed on the FYP. The structure of each JSON object is shown here. You can extract that data automatically and put it on an internal list like this:  // once the page is loaded we will search for the javascript block with the ID #SIGI_STATE and parse it's content as JSON const sigistate = await page.$eval(&quot;#SIGI_STATE&quot;, (el) =&gt; el.innerHTML); initinfo = JSON.parse(sigistate); videos = []; // Iterate through the list of videos and add it to our own data structure Object.keys(initinfo.ItemModule).forEach(async (el, c) =&gt; { videos.push(initinfo.ItemModule[el]); });   If you research requires you to get data on more videos, you have to interact with the website. When accessing the site, you will be initially prompted by a login popup, which you can close automatically.    If you are working with Chrome, you can use the developer settings to find out how you can manipulate the button with puppeteer. First open the developer console with F12, then right click the close icon, select “Inspect” and find the parent element that seems unique. In this case there is a div element with the attribute data-e2e=&quot;modal-close-inner-button&quot;. In puppeteer, you can use this attribute to identify the “X” and click it like this:  let logindialog = await page.$('div[data-e2e=&quot;modal-close-inner-button&quot;]'); // waiting until the page is loaded to the point where the dialog shows up await logindialog.click(page, 'div[data-e2e=&quot;modal-close-inner-button&quot;]'); // click on the X   After that we can scroll the page forever, tapping the key-down every two seconds.  while (true) { // this loops runs indefinitivly, if we don't close it await page.waitForTimeout(2000); // wait 2000 milliseconds await page.keyboard.press(&quot;ArrowDown&quot;); // pree the ArrowDown Key }   When watching the bot scroll through TikTok and looking at the network traffic in the developer console you’ll notice that there is a infrequent call to a TikTok API https://www.tiktok.com/api/recommend/item_list/ that returns a list of the next 30 videos. Finding this request and understanding what it is might be more complicated on other platforms or services. In some cases, undocumented APIs can be a key element of data-driven investigations to help make sense of what you are collecting.  With puppeteer, you can intercept network traffic and grab the list of recommended videos for your analysis - since it is running from within the browser you do not have to worry about any encryption. You can intercept the responses to this specific URL and process the video metadata as you like:  await page.setRequestInterception(true); // setting up pupeteer to monitor requests and responses page.on(&quot;request&quot;, (request) =&gt; request.continue()); // we do not care about requests send to servers, so we just continue on those page.on(&quot;response&quot;, (response) =&gt; { // responses are parsed by the following script try { if (response.url().indexOf(&quot;api/recommend/item_list/item_list&quot;) &gt; 0) { //if data comes from the API we have identified we process it response.json().then((data) =&gt; { // parse it as json data.itemList.forEach(async (el, c) =&gt; { // and iterate over the videos in the list to store. videos.push({ id: el.id, textExtra: el.textExtra }); }); }); console.log(&quot;Received Video Data. Seen &quot; + videos.length + &quot; so far&quot;); } } catch (e) { console.log(e); } });   You have now successfully created a scraper for the TikTok public ForYouPage with less than 50 lines of code. Review the full here  ","version":"Next","tagName":"h3"},{"title":"Storing​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#storing","content":" To analyze a specific data set over a longer period, you need to store and process it. For this example project, you can use a file-based database like node-json-db that stores everything in a single JSON file. If you plan to run a project like this on a larger scale, you should consider regular database servers like Redis or MongoDB. You can install it with npm install node-json-db similar to the installation of puppeteer above. You can than run the full code from here  Note that in this example you only store the video ID, hashtags and statistics as you do not need all the additional (personal) information and should ensure cleaning your scraped data for legal and ethical reasons.  ","version":"Next","tagName":"h3"},{"title":"Analysis​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#analysis","content":" ","version":"Next","tagName":"h2"},{"title":"Tech Setup​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#tech-setup-1","content":" To help analyze your data, consider using jupyter notebooks with python. Experience shows that miniconda works well to run python and install dependencies. You can use the conda environment published with this tutorial.  ","version":"Next","tagName":"h3"},{"title":"Import Data​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#import-data","content":" First, you can import the JSON database into a python dictionary:  import json with open('../../../tiktok-fyp-videos.json') as file: database = json.load(file) print(len(database.keys()), 'videos in database')   This will result in something like: 474 videos in database (if you let the scraper run longer or multiple times, you will of course have more videos in your dataset).  At this point you can have a look at the data you have collected so far.  import pprint pprint.pprint(next(iter(database.values())))   This will print the first video in the database. You can find an example of the structure of the video object here. It contains a lot of technical information, e.g. about the video quality and URLs of cover pictures, avatars, music and subtitles. But also the name of the author in authorID some statics in stats and of course the hashtags in the textExtra field. Let's first look at the video statistics. We can iterate over all videos and collect the views in an array. And then let the numpy package do the math:  video_views = [] for video in database.values(): video_views.append(video[&quot;stats&quot;][&quot;playCount&quot;]) average = np.mean(video_views) print(&quot;Average views:&quot;, average)   This will result in Average views: 20519932.70042194, or in plainer terms: more than 20 million views per video on average. That seems like a lot. A histogram plot shows that the data is not evenly distributed. There are a few videos with a huge number of views. Outliers from the rest.    Because of this distribution the median will therefore give a better idea of the 'average'  median = np.median(video_views) print(&quot;Median views:&quot;, median)   You can also limit the histogram plot to up to 10 million views and see that the majority of videos shown on the FYP for non-logged in web users is focused on videos that already had a large audience.    ","version":"Next","tagName":"h3"},{"title":"Hashtag Analysis​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#hashtag-analysis","content":" You can now iterate over all videos in the database to count hashtag occurrences and videos without hashtags to better understand the quality of the data.  from itertools import combinations import numpy as np videos_wo_hashtags = 0 hashtag_occurences = {} for video in database.values(): video_views.append(video[&quot;stats&quot;][&quot;playCount&quot;]) if &quot;textExtra&quot; in video: for hashtag in video[&quot;textExtra&quot;]: if hashtag[&quot;hashtagName&quot;] not in hashtag_occurences: hashtag_occurences[hashtag[&quot;hashtagName&quot;]] = 1 else: hashtag_occurences[hashtag[&quot;hashtagName&quot;]] += 1 else: videos_wo_hashtags += 1 # Use of hashtags print(&quot;There are&quot;, videos_wo_hashtags, &quot;videos without hashtags (&quot;, videos_wo_hashtags*100/len(database.keys()), &quot;%)&quot;)   You will find that 17% of the videos do not use hashtags.  If you go ahead and plot the network of hashtag combinations, you will get a graph like this:    Similarly, you can also look at the distribution of hashtags by sorting the list and printing the 20 most common hashtags.  hashtags_sorted = dict(sorted(hashtag_occurences.items(), key=lambda item: item[1], reverse=True)) print(&quot;The ten most common hashtags:&quot;) print(list(hashtags_sorted)[:20])   You see that the most common hashtags are related to TikTok itself and do not describe its content. To learn more about what type of videos are shown on the FYP you should exclude these hashtags: ['fyp', '', 'viral', 'foryou', 'foryoupage', 'fy', 'fyp シ', 'funny', 'funnyvideos', 'tiktok', 'fürdich', 'trending', 'trend', 'viralvideo']  A network graph consists of nodes (the circles) and edges (connections). Again you iterate over all videos to get a list of hashtags (nodes) and store as an edge if they occurred together on a video.  exclude_hashtags = ['fyp', '', 'foryou', 'viral', 'foryoupage', 'fypシ', 'fy', 'fürdich', 'trending', 'foryoupage', 'tiktok', 'viralvideo', 'fürdichseiteシ', '4u'] hashtag_nodes = {} hashtag_edges = [] for video in database.values(): listofhashtags = [] if &quot;textExtra&quot; in video: for hashtag in video[&quot;textExtra&quot;]: if hashtag[&quot;hashtagName&quot;] not in exclude_hashtags: listofhashtags.append(hashtag[&quot;hashtagName&quot;]) if hashtag[&quot;hashtagName&quot;] not in hashtag_nodes: hashtag_nodes[hashtag[&quot;hashtagName&quot;]]=len(hashtag_nodes.keys())+1 for combination in combinations(listofhashtags, 2): hashtag_edges.append([hashtag_nodes[combination[0]], hashtag_nodes[combination[1]]])   and visualize it with pyvis:  net = pyvis.network.Network(notebook=False, cdn_resources='local') node_count = 1 for hashtag in hashtag_nodes: net.add_node(node_count, label=hashtag) node_count+=1 net.add_edges(hashtag_edges) net.show_buttons(filter_='physics') net.show('selected_hashtags.html')     ","version":"Next","tagName":"h3"},{"title":"What to do with the results​","type":1,"pageTitle":"Data Collection and Analysis of TikTok Hashtags with JavaScript and Phyton","url":"/data-knowledge-hub/docs/data-analysis/04_00_code-samples/hashtag-analysis#what-to-do-with-the-results","content":" Congratulations you've set up a TikTok scraper that collects data on videos shown on the FYP page and visualized them in a network. You can now start to dive deep into the data and identify topic clusters or learn what topics are dominating the network. ","version":"Next","tagName":"h2"},{"title":"Example: Webscraping","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro","content":"","keywords":"","version":"Next"},{"title":"Current challenges of webscraping​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#current-challenges-of-webscraping","content":" Numerous social networks recently decided to seal off their APIs more strongly and, for example, to offer them only in exchange for payment (or not at all). This increasingly limits the possibility for scientists to systematically capture important parts of digital discourse and to examine its influence on society. The lasting consequences of such decisions are expected to become more evident in the upcoming months and years, yet an immediate effect is already observable: The relevance of webscraping is on the rise again. As a means to replace the sealed-off APIs, many programmers, especially with regard to X (Twitter), resorted to systematically retrieving data via the X website – apparently at such a high frequency that Elon Musk’s X (Twitter) decided to put all of the website’s posts behind a login wall, making it more difficult to capture X’s (Twitter) data via scraping (Binder 2023).  This is a step back: In the past, many social platforms offered an API with (often) free quotas precisely to avoid the burden of webscraping on servers (Khder 2021, p.147). Moreover, webscraping comes with a variety of ethical, technical and legal concerns and questions (see Khder 2021). Even if one is only interested in a fraction of the publicly accessible data, webscraping requires capturing the data of the entire website instead of merely extracting the relevant information: Thus, significantly more data is collected than is necessary, for example, to provide a scientific answer to a hypothesis. Images are loaded, clicks are simulated – allowing the reconstruction of individual user behavior.  While this chapter shows you the ropes of responsible webscraping, it is critical to emphasize that is primarily the providers of social networks and websites who are called upon to offer the interfaces to their data for civil society and science in a suitable manner – after all, their online presence significantly shapes public discourse. This influence must be critically monitored and analyzed, which is why monitoring for research purposes must be supported. Currently many platform operators are resisting this responsibility and the need for transparency – let’s hope the Digital Services Act will improve that situation by forcing platform operators to enable data access (see EDMO 2023; AlgorithmWatch 2023).  ","version":"Next","tagName":"h2"},{"title":"What you will learn in this chapter​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#what-you-will-learn-in-this-chapter","content":" This chapter therefore aims to highlight three things:  how to access such content, using concrete examples;how to deal with scraped data within the scope of one’s own responsibility to prevent overloading web servers andto access data responsibly and according to modern standards.  To collect data via webscraping (1), you will learn how to use the programming language R and the library rvest, using the tidy principles Wickham 2014. Intended for beginners as well as advanced programmers, this is meant to make the code presented here easy to access and analyze. Additional references, links and tutorials are included throughout.  ","version":"Next","tagName":"h3"},{"title":"Webscraping: The current state​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#webscraping-the-current-state","content":" Meanwhile numerous packages for programming languages are available, which simplify the collection of data significantly. In the Python world Beautiful Soup is one of the most important program libraries, in the R universe rvest takes a key role: Both libraries allow you to capture data from static web pages and extract elements from it.  note Other programming languages and associated libraries also enable comprehensive web scraping -- for example, Puppeteer is a good way to capture both static and dynamic web pages via Node.js. For the purposes of this article, we will take a comprehensive look at the rvest package and its capabilities -- and also point out options when, for example, pages do not have static elements but should still be captured. For an example of how to use puppeteer see this chapter in the knowledge hub.  In this chapter, you will be introduced to the rvest package and its capabilities – and learn about options on how to scrape dynamic pages.  To do so, we will use two sites as examples: the pro-Russian disinformation website Anti-Spiegel.ru of conspiracy ideologist Thomas Röper (see Balzer 2022) and the conspiracy ideology media portal AUF1 (see Scherndl and Thom 2023) run by right-wing extremist Stefan Magnet. While Röper’s Anti-Spiegel is a static website (operated with Wordpress) – i.e. a website which is rendered by the server – AUF1 is a website which, like many other modern websites, is only partially rendered and only fully loaded in the browser.  note An overview of the different principles of web content delivery can be found in this article.  Both examples are well suited to demonstrate different ways of scraping with rvest. Content-wise, they influence online discourse with anti-democratic worldviews and disinformation regarding, for example, the Russian war on Ukraine or the topic of vaccinations and pandemics. Their content continues to influence (at least parts of) society in the German-speaking world. Therefore, they not only serve as an example for data collection, but also show that it is important to deeply investigate such platforms from a societal cohesion perspective.  ","version":"Next","tagName":"h3"},{"title":"Diving into a pro-Russian disinformation world​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#diving-into-a-pro-russian-disinformation-world","content":" The Russian propagandist Thomas Röper published numerous articles on his website in which he belittles the consequences of, for example, the Russian war of aggression against the Ukraine and denies war crimes (see Journalist 2022). He not only writes his own articles, but also publishes news reports from the Russian news agency TASS and the Kremlin’s media outlet Russia Today. To determine which domains are cited particularly frequently, these sections will provide guidance on the necessary steps. Additionally, potential challenges during the scraping process will be addressed.  library(rvest) # Several packages of the **tidyverse** will be used # for example `dplyr` for data manipulation or `ggplot` to visualize the data we accessed. # Therefore, the whole tidyverse is being loaded. library(tidyverse) # Glue is used for better data annotations in the graphs. This is optional. library(glue) page &lt;- read_html(&quot;https://www.anti-spiegel.ru/category/aktuelles/&quot;)   On his website we can find a link with which we can access all his articles. rvest can make the data usable for our further analysis. Via read_html we can read a page, then we can access different elements of the web page via the function html_elements(). The easiest way is to use so called CSS selectors to choose the section which should be extracted: CSS selectors are different patterns which can be used to access different elements of an HTML page.  note An overview of different CSS selector patterns can be found at W3Schools. To make it easier to choose the right selectors, you can use the Google Chrome extension SelectorGadget: By simply selecting and deselecting on a web page, the right CSS selector for the right segment can often be found quickly, as in the example of the title of each article here. Selecting the title and deselecting the ticker shows, that currently 20 titles are being highlighted using the selector image-left .title  Selectorgadget example on Röpers page  Using the CSS selectors, you can now determine the segments that are relevant to you – afterwards, you will write a function that will store all the relevant information in one record. rvest also has useful support functions here, such as html_text(). This function allows you to convert html types into simple text vectors; html_text2() extends this function and additionally removes unnecessary white spaces.  note You can find a good introduction to rvest and documentation on its main page.  The most important functions for this examples case are html_elements() and html_attr() – they allow you to extract elements and attributes (for example, the link from the href attribute of a &lt;a&gt; node). rvest makes use of many functions of the xml2 package, so it’s worth having a look at its documentation.  note For example, html_structure() from xml2 is useful to get an overview of the structure of a web page.  page %&gt;% html_elements(&quot;.image-left .title&quot;) %&gt;% html_text()    [1] &quot;Der manipulierte Whistleblower-Bericht&quot; [2] &quot;Die UNO als Instrument des Westens&quot; [3] &quot;Der Bevölkerungsrückgang macht die Ukraine wirklich zum „Land 404“&quot; [4] &quot;Wem ist nichts mehr heilig?&quot; [5] &quot;Was Spiegel-Leser über das Treffen von Lukaschenko und Putin (nicht) erfahren&quot; [6] &quot;Droht eine Eskalation zwischen den USA und Russland?&quot; [7] &quot;Die Geschäfte der Bidens in der Ukraine&quot; [8] &quot;Der Schwarze Peter geht an Kiew&quot; [9] &quot;Käuferin von Hunter Bidens Bildern bekommt einen Posten in der US-Regierung&quot; [10] &quot;Internetkonzerne, KI und totale Zensur und Kontrolle&quot; [11] &quot;Estland verbietet Feiern zum Jahrestag der Befreiung von Nazi-Deutschland&quot; [12] &quot;Die Bidens: Eine schrecklich nette Familie&quot; [13] &quot;Die Versuche der USA, die NATO auf den Pazifik auszudehnen&quot; [14] &quot;Die BRICS laden 70 Staatschefs ein, aber niemanden aus dem Westen&quot; [15] &quot;Mein neues Buch ist jetzt im Handel&quot; [16] &quot;Putin schreibt einen Artikel über die Beziehungen zu Afrika&quot; [17] &quot;Wie die US-Demokraten die Korruptionsskandale des Biden-Clans zu verschleiern versuchen&quot; [18] &quot;Was wird aus dem Getreideabkommen?&quot; [19] &quot;Ukrainische Gegenoffensive laut Putin gescheitert: Die Ereignisse des Wochenendes&quot; [20] &quot;Unsere Fahrt an die Front in Saporoschschje&quot;   In most cases, not all content relevant to the investigation can be found on one page but will be distributed over several pages. In this example, you can find over 200 pages that can be extracted. Either way, you can either manually compile the pages you want to scrape or extract them automatically and reproducibly from the overview page.  Next page example on Röpers page  Using the CSS selector .page-numbers we can extract all page numbers - however, here we also extract the Next navigation object. To extract the relevant information from these objects (here for example the last number) you must use regular expressions. Regular expressions are certain terms and symbols that search for patterns in a text, in the following example a number that is not followed by another number.  note A useful extension for R is Regexplain. This add-on for RStudio allows to test various regular expressions directly in the interface. Many tips and tools can also be found online, for example regex101.  last_page_nmbr &lt;- page %&gt;% html_elements(&quot;.page-numbers&quot;) %&gt;% html_text() %&gt;% # Collapse the vector to one string separated by a space. paste(collapse = &quot; &quot;) %&gt;% # Regular expression for selecting the last digit in a string. str_extract(&quot;(\\\\d+)(?!.*\\\\d)&quot;)   In the next step you can now write your own function that extracts the relevant sections from each overview page. The result is output as a tibble – a data.frame object that also provides a quick overview of the data in the console and can be used for. To avoid scraping later, you should store the raw HTML in a column of the tibble. This also allows you to extract additional data, such as the number of comments, later. Such a procedure is also a best practice, as it allows you to avoid a larger server load for the site operator due to multiple scraping.  get_article_overview &lt;- function(url) { page &lt;- read_html(url) tibble( title = page %&gt;% html_elements(&quot;.image-left .title&quot;) %&gt;% html_text(), subtitle = page %&gt;% html_elements(&quot;.image-left .dachzeile&quot;) %&gt;% html_text(), url = page %&gt;% html_elements(&quot;.image-left .title&quot;) %&gt;% html_attr(&quot;href&quot;), date = page %&gt;% html_elements(&quot;.image-left .date&quot;) %&gt;% # Dates are presented in a date-mont-year format, which is common in Germany. # The locale must be installed on your computer, in this case &quot;de_DE.UTF-8&quot;. dmy(locale = &quot;de_DE.UTF-8&quot;), raw = page %&gt;% html_elements(&quot;.image-left .listing-item&quot;) %&gt;% # You want to store this as `character` because a `tibble` does not accept a `xml_nodeset` item, # which also points to a serialized object stored in memory. # You can reserialze this object again using `read_html()`. as.character() ) }   Using your just written function and the previously extracted last page number, a vector of all overview pages can now be created. Afterwards, using the function map() from the package purrr, your function can be applied to all pages and in turn and tibble can be formed from them. (map_dfr() applies the function and forms a data.frame(df) by merging the results row by row (r)).  all_articles_summary &lt;- read_rds(&quot;data/all_articles_summary.rds&quot;)   url_part &lt;- &quot;https://www.anti-spiegel.ru/category/aktuelles/?dps_paged=&quot; all_page_urls &lt;- map_chr(1:last_page_nmbr, ~paste0(url_part, .x)) all_articles_summary &lt;- map_dfr(all_page_urls, get_article_overview) all_articles_summary   Because R and rvest work only sequentially via map() the command can often take a long time: one web page is called, scraped and then the next web page is called and scraped. The internet speed, the computing power as well as possible blockades by site operators can cause individual connections to get aborted, or the command would take so long that it is no longer practicable to scrape a web page.  You can improve the scraping process by parallelization: It is important to keep in mind that parallelization leads to additional load on the affected web servers. Several pages are opened in parallel – more in the chapter on parallelization.  Webscraping is a gray area and is regulated differently from country to country or was part of court decisions. Even if site operators want to prevent web scraping, for example through their terms of use, it may still be justified and permitted, e.g. for consumer protection reasons. This is the case in Germany, for example (German Federal Court of Justice 2014).  ","version":"Next","tagName":"h2"},{"title":"Analyzing scraped data and next steps​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#analyzing-scraped-data-and-next-steps","content":" Since the data is in tidy format (see Wickham 2014), further analysis and presentation via the packages of the tidyverse is simple and clear. Using ggplot we can, for example, display the number of posts that were written per month.  all_articles_summary %&gt;% mutate(month = floor_date(date, &quot;month&quot;)) %&gt;% count(month) %&gt;% # It is recommended to filter out the current month # because it risks being misrepresented in the dataset. filter(month != today() %&gt;% floor_date(&quot;month&quot;)) %&gt;% ggplot(aes(month, n)) + geom_line() + theme_light() + labs(title = &quot;Posts per Month on antispiegel.ru&quot;, subtitle = glue(&quot;Posts until {today() %&gt;% format('%B %Y')}&quot;), y = &quot;Posts&quot;, x = NULL)   Number of article per month on the german pro-russion propaganda website antispiegel.ru by Thomas Röper  The example of Röper illustrates how to handle data from particularly active sites. While you didn’t scrape the number of comments via your function, you can do so by extracting the number of comments from the raw values in the column raw. Thus, you avoid further scraping of the data.  Note that not all articles contain comments – if comments are missing, there is also no html_element with comments, leading to fewer comment fields than e. g. title fields and a tibble cannot be constructed.  note The reason why we did this is also because not all articles contain comments – if comments are missing, there is also no html_element with comments, leading to fewer comment fields then e. g. title fields and a tibble can’t be constructed.  get_comments &lt;- function(raw_data) { element &lt;- read_html(raw_data) html_elements(element, &quot;.comments-link a&quot;) %&gt;% html_text(trim = TRUE) } article_comments &lt;- all_articles_summary %&gt;% # You should wrap this in “possibly” to prevent errors from stopping the code execution. # If you can’t extract a comment string, you give it a NA value. mutate(comments_string = map(raw, possibly(get_comments, otherwise = NA_character_))) %&gt;% unnest(comments_string) %&gt;% mutate(comments = str_extract(comments_string, &quot;\\\\d+&quot;) %&gt;% as.numeric()) article_comments %&gt;% ggplot(aes(comments)) + geom_histogram(binwidth = 1) + theme_light() + labs(title = &quot;Histogram of comments at anti-spiegel.ru&quot;, subtitle = glue(&quot;Posts until {today() %&gt;% format('%B %Y')}&quot;))   Histogram of number of comments under each article from anti-spiegel.ru  As you can see, the most discussed article has 495 (r article_comments %&gt;% top_n(1, comments) %&gt;% pull(comments)) user comments with the headline &quot;Putins Rede zur Vereinigung Russlands mit den ehemals ukrainischen Gebieten&quot; (r article_comments %&gt;% top_n(1, comments) %&gt;% pull(title)) from 01. October 2022 (r article_comments %&gt;% top_n(1, comments) %&gt;% pull(date) %&gt;% format(&quot;%d. %B %Y&quot;)). Most of the articles are not commented.  ","version":"Next","tagName":"h3"},{"title":"Scraping huge datasets via parallelization​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#scraping-huge-datasets-via-parallelization","content":" Previously, you learned how to scrape content sequentially and you were able to extract 20 article summaries per scraped page. If you now want to scrape not only the preview of the articles but all pages, the number of pages to be scrapped increases significantly.  With the package furrr you can improve the scraping process by parallelization: It is important to keep in mind that parallelization leads to additional load on the affected web servers. Several pages are opened in parallel. It is therefore important to include pauses in the function definition to distribute the load and to keep the number of cores used for parallelization low - in this case a maximum of five workers working in parallel.  Via plan(mulitsession) you specify that the following code should be executed in parallel sessions - this is not always faster because the sessions must be started and ended. Small data sets are possibly better suited with a sequential approach.  All articles of Anti-Spiegel.ru should be scrapped and analyzed – however, in first tests it could be noticed that a lot of content on Röper’s page is repeated. For example, under each article there is a reference to his book published by J.K-Fischer Verlag. However, this advertisement for his own publication is separated from the rest of the text by a dividing line.  Using simple CSS selectors, however, this part cannot be separated from the rest -- though rvest can be used with extended xpath selectors. These allow us, for example, to only scrape &lt;p&gt; nodes which are followed by a &lt;hr&gt; node (the separator).  info Cheat sheets for using xpath selectors are available here.   page &lt;- read_html(&quot;https://www.anti-spiegel.ru/2023/daenemark-deutschland-und-schweden-verweigern-auskunft-beim-un-sicherheitsrat-zur-nord-stream/&quot;) page %&gt;% html_elements(xpath = &quot;//p[following-sibling::hr]&quot;) %&gt;% html_text() %&gt;% paste(collapse = &quot;\\n&quot;)    [1] &quot;UNO, 11. Juli. TASS/ Vertreter Dänemarks, Deutschlands und Schwedens nehmen am Dienstag nicht an der von Russland einberufenen Sitzung des UN-Sicherheitsrates über die Sprengung der Gaspipelines Nord Stream und Nord Stream-2 teil, berichtet der TASS-Korrespondent.\\nZuvor hatte der erste stellvertretende ständige Vertreter Russlands bei dem Weltgremium, Dmitri Poljanski, erklärt, die russische Delegation habe für den 11. Juli eine offene Sitzung des UN-Sicherheitsrates zum Thema der Spreungung der Nord-Stream-Pipeline beantragt. Dem Diplomaten zufolge hat Russland „die britische Präsidentschaft gebeten, Vertreter“ der drei Länder – Dänemark, Deutschland und Schweden – einzuladen, die die Sabotage der Gaspipelines untersuchen, um darüber zu berichten.&quot;   Unfortunately, however, not all entries have this separator, so you won’t get results for some entries with this xpath selector – for example with this article.  page &lt;- read_html(&quot;https://www.anti-spiegel.ru/2022/ab-freitag-gibt-es-russisches-gas-nur-noch-fuer-rubel-die-details-von-putins-dekret-und-was-es-bedeutet/&quot;) page %&gt;% html_elements(xpath = &quot;//p[following-sibling::hr]&quot;) %&gt;% html_text() %&gt;% paste(collapse = &quot;\\n&quot;)    [1] &quot;&quot;   Often you will encounter such problems when webscraping, so it is important to work with a lot of examples first and test the code extensively. In this case, you scrape the data with a CSS selector and remove the always same advertising paragraphs with simple regular expressions. As mentioned above you create a function with pauses of 5 seconds (Sys.sleep(5)) on 5 processors at the same time.  all_articles_full &lt;- read_rds(&quot;data/all_articles_full.rds&quot;)   library(furrr) plan(multisession, workers = 5) get_article_data &lt;- function(url) { page &lt;- read_html(url) # Pause for 5 seconds Sys.sleep(5) # regex for addendum beginning pattern_addendum &lt;- &quot;In meinem neuen Buch.*$|Das Buch ist aktuell erschienen.*$&quot; tibble( url = url, raw = page %&gt;% html_elements(&quot;.article__content &gt; p&quot;) %&gt;% # `html_text2` to trim the data and remove unnecessary white spaces. as.character() %&gt;% str_remove(pattern_addendum) %&gt;% paste0(collapse = &quot;&quot;), text = page %&gt;% html_elements(&quot;.article__content &gt; p&quot;) %&gt;% html_text() %&gt;% str_remove(pattern_addendum) %&gt;% paste(collapse = &quot;\\n&quot;), datetime = page %&gt;% html_elements(&quot;.article-meta__date-time&quot;) %&gt;% html_text2() %&gt;% # &lt;1&gt; dmy_hm(locale = &quot;de_DE.UTF-8&quot;) ) } all_articles_full &lt;- future_map(all_articles_summary %&gt;% filter(date &gt;= &quot;2022-01-01&quot;) %&gt;% pull(url), # Wrapped in `try` to prevent errors from failing the whole process – # failed scrapings can be removed afterwards. ~try(get_article_data(.x)), .progress = TRUE)   Next, you will implement further measures to prevent your code from breaking prematurely and losing the already scraped content. Since the idea is that you wrap the function in a try, even errors do not lead to an abortion of the scraping. Errors can, however, mean connection failures. These failures can be fixed in a further process – if errors occur too often, it is recommended to optimize the code or to have longer pause times during the scraping to avoid overloading the servers.  To avoid scraping all websites, you should set clear limits for data collection, in this example this will be data from 2022 onwards – in a final step, you want to determine which domains are particularly frequently cited by Thomas Röper.  all_articles_df &lt;- all_articles_full %&gt;% keep(is_tibble) %&gt;% bind_rows() get_links &lt;- function(raw_html) { element &lt;- read_html(raw_html) element %&gt;% html_elements(&quot;a&quot;) %&gt;% html_attr(&quot;href&quot;) } get_domain &lt;- function(link) { # regex for domain domain_pattern &lt;- &quot;^(?:[^@\\\\/\\\\n]+@)?([^:\\\\/?\\\\n]+)&quot; link %&gt;% str_remove(&quot;http://|https://&quot;) %&gt;% str_remove(&quot;^www.&quot;) %&gt;% str_extract(domain_pattern) } all_articles_df %&gt;% mutate(links = map(raw, possibly(get_links, otherwise = NA_character_))) %&gt;% mutate(domain = map(links, possibly(get_domain, otherwise = NA_character_))) %&gt;% unnest(domain) %&gt;% count(domain, sort = TRUE) %&gt;% head(10) %&gt;% # `kable()` from the library `knitr` is only used in this context to generate an html table knitr::kable() # &lt;1&gt;   Table 1: Most shared domains by anti-spiegel.ru  domain\tnanti-spiegel.ru\t5793 tass.ru\t1219 spiegel.de\t611 vesti7.ru\t303 t.me\t245 vesti.ru\t191 deutsch.rt.com\t137 youtube.com\t126 kremlin.ru\t85 mid.ru\t82  To sum up, your first webscraping case highlighted that Röper refers particularly frequently to the Russian news agency TASS and the Kremlin outlet RT DE. From a research perspective, it is noteworthy that there are European sanctions and a broadcast ban against RT DE (see Spiegel 2022; Der Standard 2022) – yet our test case continues to share them (for possible explanations why, see Baeck et al. 2023). Among the most shared domains is also t.me of the platform and messenger service Telegram – this platform is used by Röper particularly often, along with YouTube. Incidentally, if you hadn’t removed the advertising block, J.K. Fischer Verlag would rank first in this table.  ","version":"Next","tagName":"h3"},{"title":"Second example: AUF1 as a dynamic website​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#second-example-auf1-as-a-dynamic-website","content":" Röper’s website is comparatively simple in design. The Wordpress site is statically generated. The web server simply outputs HTML pages, which can be downloaded and checked.  Not all sites are built this way, in fact it has probably become more common to host dynamic websites. This is also the case with the Austrian website AUF1 of the right-wing extremist and conspiracy ideologue Stefan Magnet (see Scherndl and Thom 2023), which quickly became one of the most widespread websites of pandemic deniers in the German-speaking world from autumn 2021.  Content scraped via rvest shows that reactions cannot be scraped – it simply shows an empty value.  AUF1 Screenshot with Reactions  rndm_article &lt;- &quot;https://auf1.tv/auf1-spezial/fpoe-gesundheitssprecher-kaniak-zu-who-plaenen-impf-zwang-auch-ohne-pandemie&quot; page &lt;- read_html(rndm_article) page %&gt;% html_elements(&quot;.reactions&quot;)   {xml_nodeset (0)}   A workaround here is to simulate a browser, which calls the page and executes all content. One possibility for this would be to use Selenium. This is a framework for automated software testing of web applications in different browsers –and you can use it for advanced web scraping, too.  With RSelenium you can start different browsers (to replicate this example, you should use Firefox, the corresponding binaries are downloaded via wdman). Afterwards you navigate to the corresponding page and then load the web page contents of the finished page back into rvest via read_html. Then, you can work as before and analyze different contents.  #| label: tbl-emojis #| tbl-cap: Reactions used on a random AUF1 article library(RSelenium) # start the browser # check if java and openjdk is installed first # wdman will install the rest rD &lt;- rsDriver(browser = &quot;firefox&quot;, verbose = FALSE) rD$client$navigate(rndm_article) # Setting a _sleep_ for 3 seconds to guarantee that the page has finished loading. Sys.sleep(3) # We can load in the Selenium page source directly into rvest and use it as before page &lt;- read_html(rD$client$getPageSource()[[1]]) tibble( emoji = page %&gt;% html_elements(&quot;.reactions .emoji&quot;) %&gt;% html_text() %&gt;% unique(), count = page %&gt;% html_elements(&quot;.reactions .count&quot;) %&gt;% html_text() %&gt;% as.integer() ) %&gt;% arrange(-count) %&gt;% pivot_wider(names_from = emoji, values_from = count) %&gt;% # Just for styling purposes, this looks more clean in the browser knitr::kable(align = &quot;c&quot;)   Table 2: Reactions used on a random AUF1 article  👍🏻\t❤️\t💪🏻\t😮\t😢489\t229\t131\t64\t58  With RSelenium you can extract data which wouldn’t be possible via rvest alone. You could now write a function to extract all emojis on videos and find the video that had the most interactions – this is not something, we will show in this chapter, but should be provided for interested (social) scientists and civil society researchers to build webscraping projects on their own. Even for advanced and dynamic web pages.  One last thing: Since you are using an automated browser on Desktop, you need to close it after reading in your data. Otherwise, the browser will keep on running in the background. You should also close your server that provided the background tasks for running Selenium.  rD$client$close() rD$server$stop()   ","version":"Next","tagName":"h3"},{"title":"Literature​","type":1,"pageTitle":"Example: Webscraping","url":"/data-knowledge-hub/docs/data-collection/03_00_code-samples/web-scraping-intro#literature","content":" AlgorithmWatch. (2023). DSA Must Empower Public Interest Research with Public Data Access. AlgorithmWatch. https://algorithmwatch.org/en/dsa-empower-public-interest-research-data-access/  Baeck, Jean-Philipp, Fromm, Anne, &amp; Peters, Jean. (2023). Warum Russia Today trotz Sanktionen in Europa weiterläuft. correctiv.org. https://correctiv.org/aktuelles/russland-ukraine-2/2023/02/17/eu-sanktionen-gcore-russia-today/  Balzer, Erika. (2022). Desinformations-Medien: Der Anti-Spiegel - Russische Propaganda und Verschwörungsmythen. Belltower.News. https://www.belltower.news/desinformations-medien-der-anti-spiegel-russische-propaganda-und-verschwoerungsmythen-132357/  Binder, Matt. (2023). Elon Musk Claims Twitter Login Requirement Just 'Temporary'. Mashable. https://mashable.com/article/elon-musk-twitter-login-requirement-temporary  DerStandard. (2022). Bis zu 50.000 Euro Strafe für Verbreitung von russischem Staatssender RT – alleine FPÖ stimmt dagegen. DER STANDARD. https://www.derstandard.de/story/2000133980411/bis-50-000-euro-fuer-verbreitung-von-russischem-staatssender-rt  EDMO. (2023). Members of the EDMO Task Force on Disinformation on the War in Ukraine Submit Feedback to the EC Call for Evidence on the Provisions in the DSA Related to Data Access. EDMO. https://edmo.eu/2023/05/30/members-of-the-edmo-task-force-on-disinformation-on-the-war-in-ukraine-submit-feedback-to-the-ec-call-for-evidence-on-the-provisions-in-the-dsa-related-to-data-access/  Fung, Brian. (2023a). Academic Researchers Blast Twitter's Data Paywall as 'outrageously Expensive'. CNN Business. https://www.cnn.com/2023/04/05/tech/academic-researchers-blast-twitter-paywall/index.html  Fung, Brian. (2023b). Reddit Sparks Outrage after a Popular App Developer Said It Wants Him to Pay $20 Million a Year for Data Access. CNN Business. https://www.cnn.com/2023/06/01/tech/reddit-outrage-data-access-charge/index.html  German Federal Court of Justice. (2014). BGH, 30.04.2014 - I ZR 224/12. BGH.  Glez-Peña, Daniel, Lourenço, Anália, López-Fernández, Hugo, Reboiro-Jato, Miguel, &amp; Fdez-Riverola, Florentino. (2014). Web Scraping Technologies in an API World. Briefings in Bioinformatics. https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbt026  Journalist. (2022). Im Desinformationskrieg. journalist.de. https://www.journalist.de/startseite/detail/article/im-desinformationskrieg  Khder, Moaiad Ahmad. (2021). Web Scraping or Web Crawling: State of Art, Techniques, Approaches and Application. International Journal of Advances in Soft Computing &amp; Its Applications, 13(3).  Scherndl, Gabriele, &amp; Thom, Paulina. (2023). Was hinter dem österreichischen Verschwörungssender Auf1 steckt. correctiv.org. https://correctiv.org/faktencheck/hintergrund/2023/04/27/was-hinter-auf1-stefan-magnet-und-der-ausbreitung-des-oesterreichischen-verschwoerungssenders-steckt-desinformation-und-rechte-hetze/  Spiegel. (2022). Maßnahmen gegen russische Staatsmedien: Verbreitung von RT und Sputnik ist in der EU ab sofort verboten. Der Spiegel. https://www.spiegel.de/kultur/ukraine-krieg-verbreitung-von-rt-und-sputnik-ist-in-der-eu-ab-sofort-verboten-a-49597add-c2b2-44da-b1e4-6832d3ea824f  Wickham, Hadley. (2014). Tidy Data. The Journal of Statistical Software. http://www.jstatsoft.org/v59/i10/ ","version":"Next","tagName":"h2"},{"title":"Common data collection methods on social media platforms illustrated with TikTok","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods","content":"","keywords":"","version":"Next"},{"title":"Open Source Research (aka Document Audit)​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#open-source-research-aka-document-audit","content":" Open-Source Research can cover a range of sources. From public documentation shared by platforms themselves, to reports or information they are legally required to publish to internal documents that become available due to leaks or unintended publications.  ","version":"Next","tagName":"h2"},{"title":"Press Releases​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#press-releases","content":" Online platforms and services often share information and updates for marketing purposes, highlighting positive things like user growth or examples of how they support the individual or public good. TikTok for example regularly publishes updates on their own “newsroom” about new products as well as analyses of the content on the platform (e.g. Toplists). While press releases are commonly favorable to the platform, they also intend to shape public discourse about the platform (or disperse criticism). For example, in the TikTok Community News section you will find that TikTok specifically mentions several communities from Entertainment (Gaming, Sports, Music), but also stories with which TikTok wants to emphasize diversity after - presumably to counter narratives that certain voices are being silenced or that negative content is spreading on the platform. This includes articles hightlighting #BlackTikTok, #WomenOfTikTok, #TransVisibility or Medical #MentalHealthAwarness #ItsTimeForChange (Eating Disorder Awareness Week).  I asked ChatGPT to “identify 5 recurring topics in the following text”, which was the 33 headlines of 2023 I copy pasted from the website. chatGPT classfied the headlines into the following topics, that could be intrepreted as what the TikTok PR Department thinks is most important to the brand:  Gaming on TikTokCommunity and CreativityPartnerships and CollaborationsMusic and EntertainmentSocial and Cultural Awareness  ","version":"Next","tagName":"h3"},{"title":"Documentation and Reports​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#documentation-and-reports","content":" Another source of information are official reports or (legal) documentaion, including Terms of Services, support documents, transparency reports, audits or similar. TikTok for example publishes transparency reports on their website. They often provide data in a machine-readable format so that it is possible to track changes over time or conduct analyses beyond what the platform shares proactively.  ","version":"Next","tagName":"h3"},{"title":"Research​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#research","content":" Large online platforms often have dedicated research and development teams that publish some of their findings in journals or share results during scientific conferences. Google for instance has published some very impactful research, starting with the original page rank algroithm on which the platform was based in its early years, as well as various contributions to research in the AI field, most notably the transformer models that enabled the creation of large language models. Though academic research papers often focus on a rather narrow question, they sometimes explain a key element of a platform in depth (like the page rank example).  Not all platforms are open about the inventions at the core of their systems, but even ByteDance, the parent company of TikTok, allows their employees to publish academic papers from time to time. In 2022, TikTok researchers published a study Monolith: Real Time Recommendation System With Collisionless Embedding Table, which in which they describe how they tackled the challenges of real-time recommendations in large item space. While it is unclear, whether this is the indeed the underlying technology that drives TikTok, many assume it is.  ","version":"Next","tagName":"h3"},{"title":"Internal Documents and Journalistic Sources​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#internal-documents-and-journalistic-sources","content":" Investigative journalists sometimes have access to internal information from whistleblowers and informants that reveal organizational misconduct or systemic problems within platforms.  TikTok has been subject of whistleblowing and leaking, too. In June 2022 audio recordings from meetings were leaked to reporters that disclosed that data from US users was being accessed from China. Early in 2023 the same journalists revealed that TikToks For You Page is not entirely driven by the algorithm, but that instead TikTok employees can use a mechanism coined 'heating' to push certain content.  ","version":"Next","tagName":"h3"},{"title":"Document Leaks​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#document-leaks","content":" Sometimes parts of the material used for journalistic reporting become public, too. Platform researchers can use these documents to obtain first-hand knowledge about a platform and its inner workings.  In 2021, someone discovered a leaked document from TikTok breaking down the algorithm. The New York Times reported about the incident but did not give access to their translation. We optained the orgiginal document from chinese document sharing platforms and translated it to get a first hand look at the document.  During research for our auditing project, we discovered the original Chinese-language document and used google translate to translate it for research purposes. It containes a detailed description of the idea of the algorithms used for ranking including the purposes and underlying ‘values’ that drive the development at TikTok.  ","version":"Next","tagName":"h3"},{"title":"Code/data audit:​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#codedata-audit","content":" You can, of course, also apply more technical approaches to platforms and run analyses based on code, such asopen source code or reverse engineering specific applications or platform architectures.  ","version":"Next","tagName":"h2"},{"title":"Open Source Code​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#open-source-code","content":" While there are few open sources packages by TikTok itself, you can find a selection on Github, for example in the ByteDance repositories. Besides data related to research (see above), you can find documentation for Software Development Kit (SDK) that other apps can use to share data with TikTok as well as libraries hinting at the architecture used at ByteDance in general or TikTok (or is chinese equivalent Douyin) specifically.  Other platforms are more open about the code they develop and the products they use. For example, X (Twitter) has published a version of the code of their recommendation engine, although responses about the transparency it offers are mixed  On a more technical level, Facebook with react and zstd and Google with a variety of contributions have also openly contributed and fostered open source projects and research which also, in part, power their own platforms.  ","version":"Next","tagName":"h3"},{"title":"Reverse Engineering​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#reverse-engineering","content":" Reverse Engineering is a process through which researchers try to understand the logic of a program, service or app that is in some way obfuscated. For instance, this could mean that the code is compiled and the high-level logic is already translated in low-level code – which can make it harder for humans to understand. In other cases, developers intentionally obfuscate code to prevent others from replicating their work, notably when no compiling is involved, as is often the case on the web where JavaScript Code and HTML are normally provided in a form that are reproducible.  Reverse Engineering is helpful when analyzing apps like TikTok. For Android Apps, MobSF provides reverse engineering and other security tools. For example, these tools automatically scan the code for known trackers and libraries. In addition, MobSF can help provide the results of common Android decompiling tools.    ","version":"Next","tagName":"h3"},{"title":"API Access​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#api-access","content":" Many platforms allow researchers to access their platforms, or specific subsystems of their platforms, through APIs. While data access through APIs is subject to changes at the platform’s digression, very large online platforms operating in the European Union are legally required to provide APIs for public content. While X (Twitter) and Reddit are currently lacking in compliance, other platforms like Telegram or YouTube are still accessible through APIs. Moreover, there are third party platforms like RapidAPI that provide “unofficial APIs” for several networks – please do be mindful how and for what research purposes you access these.  ","version":"Next","tagName":"h2"},{"title":"Hidden APIs​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#hidden-apis","content":" Web based platforms offer a simple way to peak into their inner workings. Familiarize yourself with the respective developer tools to understand how a platform's API and REST works. You can use the network panel in the developer tools to see the &quot;[hidden APIs]&quot;(We https://ianlondon.github.io/blog/web-scraping-discovering-hidden-apis/), the messages exchanged between your browser and the platform that contain the data shown to you, mostly in a structured format.  The Markup has a great walkthrough on how they used APIs to uncover a story. If you want to dig a bit deeper and don't want to check all APIs manually you can use tools like mitmproxy2swagger to systematically analyse a website. To do so, you browse a website with the developer tools, open the network tab and surf the website. Afterwards you can download the HAR file as described in the mitmproxy2swagger documentation and use the following command to create a systematic description of the API.  mitmproxy2swagger -i ~/Downloads/www.tiktok.com.har -o ~/Downloads/tt_web_api.yml -p &quot;https://www.tiktok.com/api/&quot;  The YAML file you will receive is an API definition in a style calld &quot;swagger&quot;. If you paste the data to an appropriate editor you will get an overview of the data, that the tiktok web application sends and receives:  ","version":"Next","tagName":"h3"},{"title":"Data Donations​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#data-donations","content":" To understand the experience of actual users of the platforms, first-hand data often provides you with the richest and most illustrative insight.  To collect and analyze user data, you can revert to data donations. Data donations are sensitive and therefore require researchers to be rigorous, transparent, and accountable for how they use the data that they ask users to hand over. Today, users can often access data that a specific platform has stored about them – in the European Union by filing a data access request based on the General Data Protection Regulation (GDPR) . For TikTok, a data donation approach has been used by the DataSkope project as well as by academic researchers, who recruited participants via Facebook and then asked them to share their TikTok data, results are captured in Likes and Fragments: Examining Perceptions of Time Spent on TikTok.  This method comes with certain drawbacks: you need to consider cost as well as size of your needed data set. Moreover, for TikTok specifically, researchers found that the data that users can download only presents a selection of the data the platform does in fact collect.  ","version":"Next","tagName":"h2"},{"title":"Scraping​","type":1,"pageTitle":"Common data collection methods on social media platforms illustrated with TikTok","url":"/data-knowledge-hub/docs/data-collection/03_02_data-collection-methods#scraping","content":" Another common method for gathering information about a platform and content that is published on it is scraping. Scraping is a way of extracting data from websites or apps in a structured form to replicate the content available on a platform. This allows researchers to gain insights into different aspects such as: Networks of actors, content published on specific topic, filtering or prioritization mechanisms to present content on the platform and more. For example, we have scraped the public TikTok website to better understand the topics or pieces of content that are going “viral” in Germany in a week-by-week analysis. Other researchers have leveraged scraping to understand the development in different countries or learn more about TikTok’s approach to blocking and deleting content. An in-depth how to guide for web scraping can be found on this Data Knowledge Hub. ","version":"Next","tagName":"h2"},{"title":"Literature and illustrative research","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/literature/","content":"Literature and illustrative research Literature Overview: This chapter offers an academic synthesis of the current scientific discourse around social media monitoring. By combining insights from journals, publications, and key policy documents, it helps you read up on foundational references and get a sense of the academic landscape in this field. Open for contributions We welcome contributions on a rolling basis. At the moment, we particularly welcome chapters dealing with the following questions Exploration of existing monitoring projectsResearch Gaps and Call4Action","keywords":"","version":"Next"},{"title":"Introduction to researching and monitoring online discourse","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/get-started/","content":"Introduction to researching and monitoring online discourse To help you get started and better navigate the research landscape of social media monitoring (SMM) and researching online discourse more broadly, we’ve collected a first set of chapters that aim to provide orientation and guidance to researchers and practitioners exploring various options and dimensions of social media monitoring. We welcome additional content and have added a selection of chapters below that would make for great contributions. Ethical Considerations: This chapter critically examines the ethical challenges inherent in social media monitoring and data sharing practices. Drawing on the Open Knowledge Foundation’s (OKFN) extensive experience in data ethics, it outlines a set of standards rooted in principles such as privacy, transparency, and accountability. The emphasis is placed on the significance of a privacy-centric design approach to promote ethical data management. Legal Considerations: This chapter provides an overview of the legal parameters that govern the monitoring of social media platforms. By presenting a clear outline of the legal context and incorporating relevant IT considerations, it provides a starting point for researchers and practitioners to ensure lawful engagement with these platforms. Open for contributions We welcome contributions on a rolling basis. At the moment, we particularly welcome chapters dealing with the following questions How to deal with &quot;dark socials&quot;?Data access rights beyond the European Union and the U.S.","keywords":"","version":"Next"},{"title":"Read up on social media monitoring: What does the literature say?","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview","content":"","keywords":"","version":"Next"},{"title":"State of the Art: Research on mis- and disinformation​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#state-of-the-art-research-on-mis--and-disinformation","content":" The following article provides an overview on the current debate in the field of social media monitoring with a focus on mis- and disinformation and intends to serve as a starting point for further research and more in-depth reading. The literature review was conducted based on a screening of recent publications from leading journals (e.g., Political Communication, Nature Human Behaviour), widely cited literature published in the last 15 years, as well as current policy papers and reports from relevant institutions and actors in the field of social media monitoring.  This overview subsequently discusses the role of social media in circulating disinformation, before describing how social media monitoring is employed to track and engage with disinformation.  ","version":"Next","tagName":"h2"},{"title":"Premise of social media​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#premise-of-social-media","content":" Social media has become a popular playground for various actors seeking to inject false or misleading information into the public information stream – from intelligence agencies over (extremist) political parties to rogue civil society. Simultaneously, social media is increasingly used as an important source of information for citizens around the globe.  ","version":"Next","tagName":"h3"},{"title":"The (dis-)information ecosystem: terminology and definitions​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#the-dis-information-ecosystem-terminology-and-definitions","content":" While well-informed citizens are the cornerstone of a functioning democracy (Lewandowsky et al., 2020), false as well as misleading information threatens to undermine this informational basis citizens depend on. These types of information are frequently referred to as mis- and/or disinformation. While misinformation is merely false information, for example reporting errors in journalistic pieces, disinformation intentionally seeks to mislead individuals (Wardle &amp; Derakhshan, 2017). Beyond that, various other sub-types of disinformation are prevalent online. These include, for instance, conspiracy theories often linked to beliefs that actors with malign intent operate in secret against the public (for an overview see, Douglas &amp; Sutton, 2023), as well as “fake news”, which are published in the style of legitimate news articles but are partially or fully fabricated (Tandoc et al., 2018).  ","version":"Next","tagName":"h3"},{"title":"How and why mis- and disinformation spreads​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#how-and-why-mis--and-disinformation-spreads","content":" While the pursuit of a political agenda is an important reason for people to circulate conspiracy theories (Douglas &amp; Sutton, 2023), fake news are more frequently shared due to inattention of users rather than the intention to mislead (Pennycook &amp; Rand, 2021). Disinformation is also not limited to simple texts, tweets, or posts. Technological progress made it possible to alter pictures and videos in a way that serves disinformation purposes. Fabricated videos are now commonly referred to as deepfakes (Chesney &amp; Citron, 2018). This overview subsequently discusses the role of social media in circulating disinformation, before describing how social media monitoring is employed to track and engage with disinformation.  ","version":"Next","tagName":"h3"},{"title":"Social media as vector for disinformation​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#social-media-as-vector-for-disinformation","content":" Very broadly, social media can be understood as Internet-based channels in which users can interact and share content, either in real-time or asynchronously (Carr &amp; Hayes, 2015). Various social media platforms exist today, such as Discord, Facebook, Flickr, Instagram, LinkedIn, Pinterest, Reddit, Snapchat, Telegram, TikTok, Twitter, and YouTube. Virtual social and virtual game worlds can also be understood as social media, such as Minecraft or the Metaverse (Kaplan &amp; Haenlein, 2010).  These platforms have different audiences, purposes, and functionalities, creating a complex and continuiously evolving media landscape. To understand the of digital media use, including social media, on democracy, the systematic review by Lorenz-Spreen et al. (2022) is highly recommended. Regardless of their purpose or funtionalities, disinformation is prevalent on most, if not all, platforms. For example, Facebook, Instagram, Reddit, and Twitter all struggle to contain disinformation on their platforms (Hao, 2021; Lukito, 2020; The Economist, 2020; Yang et al., 2021). While these platforms are used to dissiminate disinformation, some platforms are used to prepare and develop disinformation, including conspiracy theories, such as the messaging board 4chan in the case of Pizzagate (Tuters et al., 2018).  Beyond that, individuals frequently use several social media platforms, connecting these technically separate spheres. As a result, disinformation can reemerge at a different point in time on other platforms (Kang &amp; Frenkel, 2020). Thus, disinformation campaigns spread across various platforms (Lukito, 2020), often with tailored content to maximize engagement and visibility, and can therefore reach a large number of people – sometimes in surprising places. For instance, Russian propaganda has recently been reported to be present in various online video games, such as Minecraft (Myers &amp; Borwning, 2023).  This is especially problematic in crises, e.g., terrorist attacks, wars, or natural disasters, or otherwise sensitive situations, such as elections. During these times, many people go on social media to follow the developing situation and try to find the latest information. Monitoring these developments on social media is therefore of crucial importance (Reuter &amp; Kaufhold, 2018; Starbird et al., 2014; Vieweg et al., 2010).  ","version":"Next","tagName":"h2"},{"title":"Monitoring disinformation on social media​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#monitoring-disinformation-on-social-media","content":" Social media monitoring (SMM) describes the systematic observation of social media platforms and other digital information sources. Although various approaches to SMM exist, the process can be broadly organized in four steps (Brady, 2020; Karafillakis et al., 2021):  preparation (e.g., defining the problem or goal of the monitoring as well as associated topics and terms),data extraction,data analysis,and dissemination of findings.  Some contexts demand a more comprehensive planning step, that includes a risk assessment to better understand relevant factors such as trust in media, social media consumption, and the political landscape (Brady, 2020). Data collection is to a large extent automatized by crawling a social media platform to retrieve data and identify trends in respect to a certain set of topics or keywords but also specific sources of information. As Brady however points out based on insights from SMM during five European elections, there is no gold standard for SMM, and the field remains largely experimental.  Thus, the precise combination of tools, team qualifications, and organizational settings varies from case to case. The increasing availability of AI technology will also support SMM. Some argue, that given the immense volume of data produced every day, only AI will be able to identify emerging threats in real time (Yankoski et al., 2020). Research on automated fake news detection highlights the potential of this approach (for example, Tacchini et al., 2017).  In the context of disinformation, SMM is primarily employed to protect elections and address disinformation campaigns. Monitoring the information space during elections has become increasingly important. For instance, Russia tried to interfere in the 2016 U.S. presidential elections, although with limited success in respect to changing people’s attitudes or voting behavior (Eady et al., 2023). Various elections of EU member states were targeted by disinformation campaigns as well, such as Slovakia in 2019 and Spain in 2019 and again in 2023 (Fundación Maldita.es &amp; Democracy Reporting International, 2023), although mostly with limited success (Bayer et al., 2021). Another notable case was the Brazilian elections in 2022, which witnessed a comprehensive disinformation campaign, using various tools and channels such as Cheapfakes (i.e., altered images and audio based on off-the-shelf technology) and WhatsApp Status. The campaign aimed to undermine citizens' trust in the elections and democratic institutions in general (Saab et al., 2022). Brady (2020) provides an informative example for SMM during elections.  SMM is also employed by organizations dedicated to counter ongoing foreign interference aimed at undermining public trust in democratic institutions. Distinguishing organic rumors from organized campaigns is a challenging endeavor, as disinformation campaigns often blend false information with facts (Starbird, 2020). Examples of relevant organizations include the NATO Strategic Communications Centre of Excellence (StratCom COE) located in Latvia (for a brief introduction of StratCom COE, see Hanley, 2022) as well as the European External Action Services’ East StratCom Task Force (ESCTF). The flagship product of ESCTF is the EUvsDisinfo initiative, which collects and debunks disinformation campaigns targeting the EU, its member states, and neighboring countries.  ","version":"Next","tagName":"h2"},{"title":"Making SMM-based research actionable​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#making-smm-based-research-actionable","content":" Monitoring disinformation and similar phenomena is of crucial importance to protect democratic processes and institutions. However, given the versatile category of disinformation, that is not limited to text but increasingly includes videos and audio (for example, deepfakes (Chesney &amp; Citron, 2018; Weikmann &amp; Lecheler, 2022)), and the everchanging social media landscape, SMM faces significant challenges.  From this perspective, SMM should be intertwined with comprehensive outreach and community mobilization efforts, to increase the effectiveness of SMM and disseminate findings to a larger public as fast as possible. Thus, like science communication (Holford et al., 2023), SMM should be understood as a collective intelligence endeavor that builds on the technical and regional expertise of various stakeholders, as illustrated by Brady's (2020) examples as well as the recent monitoring of Spain’s snap general election (Fundación Maldita.es &amp; Democracy Reporting International, 2023). SMM therefore goes beyond merely monitoring the information space on social media, as it tries to contribute to a better understanding of the current challenges democratic information spheres are coping with.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Read up on social media monitoring: What does the literature say?","url":"/data-knowledge-hub/docs/literature/05_02_literature-overview#references","content":" Bayer, J., Holznagel, B., Lubianiec, K., Pintea, A., Schmitt, J. B., Szakács, J., Uszkiewicz, E., European Parliament. Directorate-General for External Policies of the Union. Policy Department, &amp; European Parliament. Special Committee on Foreign Interference in all Democratic Processes in the European Union, including D. (2021). Disinformation and propaganda: impact on the functioning of the rule of law and democratic processes in the EU and its Member States - 2021 update. European Parliament. https://www.europarl.europa.eu/meetdocs/2014_2019/plmrep/COMMITTEES/INGE/DV/2021/04-13/EXPO_STU2021653633_EN.pdf  Brady, M. (2020). Lessons Learned: Social Media Monitoring during Elections. Democracy Reporting International. https://democracy-reporting.org/en/office/global/collection?type=publications  Carr, C. T., &amp; Hayes, R. A. (2015). Social Media: Defining, Developing, and Divining. Atlantic Journal of Communication, 23(1), 46–65. https://doi.org/10.1080/15456870.2015.972282  Chesney, R., &amp; Citron, D. K. (2018). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security (No. 692; Public Law Research Paper). https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954  Douglas, K. M., &amp; Sutton, R. M. (2023). What Are Conspiracy Theories? A Definitional Approach to Their Correlates, Consequences, and Communication. Annual Review of Psychology, 74(1), 271–298. https://doi.org/10.1146/annurev-psych-032420-031329  Eady, G., Paskhalis, T., Zilinsky, J., Bonneau, R., Nagler, J., &amp; Tucker, J. A. (2023). Exposure to the Russian Internet Research Agency foreign influence campaign on Twitter in the 2016 US election and its relationship to attitudes and voting behavior. Nature Communications, 14(1). https://doi.org/10.1038/s41467-022-35576-9  Fundación Maldita.es &amp; Democracy Reporting International. (2023, August 8). Disinformation and Hate Online During the Spanish Snap General Election. Democracy Reporting International. https://democracy-reporting.org/en/office/global/publications/disinformation-and-hate-online-during-the-spanish-snap-general-election-of-july-2023  Hanley, M. (2022). NATO’s Response to Information Warfare Threats. In J. Chakars &amp; I. Ikmanis (Eds.), Information Wars in the Baltic States (pp. 205–223). Palgrave Macmillan. https://doi.org/10.1007/978-3-030-99987-2_11  Hao, K. (2021). How Facebook got addicted to spreading misinformation. MIT Technology Review. https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/  Holford, D., Fasce, A., Tapper, K., Demko, M., Lewandowsky, S., Hahn, U., Abels, C. M., Al-Rawi, A., Alladin, S., Sonia Boender, T., Bruns, H., Fischer, H., Gilde, C., Hanel, P. H. P., Herzog, S. M., Kause, A., Lehmann, S., Nurse, M. S., Orr, C., … Wulf, M. (2023). Science Communication as a Collective Intelligence Endeavor: A Manifesto and Examples for Implementation. Science Communication, c. https://doi.org/10.1177/10755470231162634  Kang, C., &amp; Frenkel, S. (2020, June 27). ‘PizzaGate’ Conspiracy Theory Thrives Anew in the TikTok Era. The New York Times, 30–33. https://www.nytimes.com/2020/06/27/technology/pizzagate-justin-bieber-qanon-tiktok.html  Kaplan, A. M., &amp; Haenlein, M. (2010). Users of the world, unite! The challenges and opportunities of Social Media. Business Horizons, 53(1), 59–68. https://doi.org/10.1016/j.bushor.2009.09.003  Karafillakis, E., Martin, S., Simas, C., Olsson, K., Takacs, J., Dada, S., &amp; Larson, H. J. (2021). Methods for social media monitoring related to vaccination: Systematic scoping review. JMIR Public Health and Surveillance, 7(2). https://doi.org/10.2196/17149  Lewandowsky, S., Smillie, L., Garcia, D., Hertwig, R., Weatherall, J., Egidy, S., Robertson, R. E., O’Connor, C., Kozyreva, A., Lorenz-Spreen, P., Blaschke, Y., &amp; Leiser, M. (2020). Technology and Democracy: Understanding the influence of online technologies on political behaviour and decision-making. https://doi.org/10.2760/709177  Lorenz-Spreen, P., Oswald, L., Lewandowsky, S., &amp; Hertwig, R. (2022). A systematic review of worldwide causal and correlational evidence on digital media and democracy. Nature Human Behaviour, 7(1), 74–101. https://doi.org/10.1038/s41562-022-01460-1  Lukito, J. (2020). Coordinating a Multi-Platform Disinformation Campaign: Internet Research Agency Activity on Three U.S. Social Media Platforms, 2015 to 2017. Political Communication, 37(2), 238–255. https://doi.org/10.1080/10584609.2019.1661889  Myers, S. L., &amp; Borwning, K. (2023, July 30). Russia Takes Its Ukraine Information War Into Video Games. The New York Times. https://www.nytimes.com/2023/07/30/technology/russia-propaganda-video-games.html#:~:text=Russian propaganda is spreading into,popular social media network%2C VKontakte.  Pennycook, G., &amp; Rand, D. G. (2021). The Psychology of Fake News. Trends in Cognitive Sciences, 25(5), 388–402. https://doi.org/10.1016/j.tics.2021.02.007  Reuter, C., &amp; Kaufhold, M.-A. (2018). Fifteen years of social media in emergencies: A retrospective review and future directions for crisis Informatics. Journal of Contingencies and Crisis Management, 26(1), 41–57. https://doi.org/10.1111/1468-5973.12196  Saab, B. A., Beyer, J. N., &amp; Böswald, L.-M. (2022). Beyond the Radar: Emerging Threats, Emerging Solutions. Democracy Reporting International. https://democracy-reporting.org/en/office/global/publications/going-beyond-the-radar-emerging-threats-emerging-solutions  Starbird, K. (2020, July). Disinformation campaigns are murky blends of truth, lies and sincere beliefs – lessons from the pandemic. The Conversation. https://theconversation.com/disinformation-campaigns-are-murky-blends-of-truth-lies-and-sincere-beliefs-lessons-from-the-pandemic-140677  Starbird, K., Maddock, J., Orand, M., Achterman, P., &amp; Mason, R. M. (2014, March 1). Rumors, False Flags, and Digital Vigilantes: Misinformation on Twitter after the 2013 Boston Marathon Bombing. IConference 2014 Proceedings. https://doi.org/10.9776/14308  Tacchini, E., Ballarin, G., Della Vedova, M. L., Moret, S., &amp; de Alfaro, L. (2017). Some like it Hoax: Automated fake news detection in social networks. CEUR Workshop Proceedings, 1960, 1–12.  Tandoc, E. C., Lim, Z. W., &amp; Ling, R. (2018). Defining “Fake News”: A typology of scholarly definitions. Digital Journalism, 6(2), 137–153. https://doi.org/10.1080/21670811.2017.1360143  The Economist. (2020). Instagram will be the new front-line in the misinformation wars. https://www.economist.com/the-world-in/2020/01/01/instagram-will-be-the-new-front-line-in-the-misinformation-wars  Tuters, M., Jokubauskaitė, E., &amp; Bach, D. (2018). Post-Truth Protest: How 4chan Cooked Up the Pizzagate Bullshit. M/C Journal, 21(3), 1–18. https://doi.org/10.5204/mcj.1422  Twetman, H., Paramonova, M., &amp; Hanley, M. (2020). Social Media Monitoring: A Primer. Methods, tools, and applications for monitoring the social media space. NATO Strategic Communications Centre of Excellence. https://stratcomcoe.org/pdfjs/?file=/cuploads/pfiles/social_media_monitoring_a_primer_12-02-2020.pdf?zoom=page-fit. Consultado em 01mai2023, 18:10  Vieweg, S., Hughes, A. L., Starbird, K., &amp; Palen, L. (2010). Microblogging during two natural hazards events. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2, 1079–1088. https://doi.org/10.1145/1753326.1753486  Wardle, C., &amp; Derakhshan, H. (2017). Information Disorder: Toward an Interdisciplinary framework for research and policy making. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-framework-for-researc/168076277c  Weikmann, T., &amp; Lecheler, S. (2022). Visual disinformation in a digital age: A literature synthesis and research agenda. New Media &amp; Society. https://doi.org/10.1177/14614448221141648  Yang, K. C., Pierri, F., Hui, P. M., Axelrod, D., Torres-Lugo, C., Bryden, J., &amp; Menczer, F. (2021). The COVID-19 Infodemic: Twitter versus Facebook. Big Data and Society, 8(1). https://doi.org/10.1177/20539517211013861  Yankoski, M., Weninger, T., &amp; Scheirer, W. (2020). An AI early warning system to monitor online disinformation, stop violence, and protect elections. Bulletin of the Atomic Scientists, 76(2), 85–90. https://doi.org/10.1080/00963402.2020.1728976 ","version":"Next","tagName":"h2"},{"title":"Ethical Considerations","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations","content":"","keywords":"","version":"Next"},{"title":"Planning checklist: 7 principles for handling social media data ethically​","type":1,"pageTitle":"Ethical Considerations","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations#planning-checklist-7-principles-for-handling-social-media-data-ethically","content":" 1. Anonymisation: Anonymise data to protect individuals' privacy by replacing personal identifiers with random codes.  2. Data Minimisation: Avoid collecting unnecessary or sensitive information by only collecting data directly relevant to your research purposes.  3. Data Retention: Delete data when it is no longer needed.  4. Safe Data Storage: Ensure proper encryption protocols are used during transmission and that collected data is stored securely.  5. Harm Mitigation: Consider social, psychological and local legal consequences that can harm individuals or communities with the results of research.  6. Advocacy: Use social media as a tool for responsible advocacy and minimise the data storage and collection about your advocacy and campaigns.  7. Research Funding Transparency: if you conduct funded research, be transparent about the funding sources and potential conflicts of interest that could influence it. An improved level of transparency will likely increase trustworthiness of the way proposals are handled and of the grant allocation system in general.  ","version":"Next","tagName":"h2"},{"title":"Context: Data availability, personal data, access rights​","type":1,"pageTitle":"Ethical Considerations","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations#context-data-availability-personal-data-access-rights","content":" After two decades of a technological revolution resulting in unprecedented Internet connectivity and social media adoption, technology companies have become increasingly powerful and opaque in how they exercise such power. ​​Despite having become critical to people’s lives, citizens have almost no vehicle to request information and data about how social media platforms operate, beyond the right to access data under GDPR.  Currently, there is no equivalent of access to information laws to request information and data sets shaping our digital lives on social platforms, as the data released by social media companies is limited. Only eventually, through research, whistleblowing, public hearings or legal discovery processes, such fragments of information come into the light.  When there is no accessible data, public interest research conducted by non-governmental organisations permits an understanding of how social media companies operate or how they shape specific topics, areas, or events. Social and digital activists often collect such data by open web scraping, crowdsourcing, or other innovative technology methods. However, such activities often confront them with legal questions and ethical dilemmas related to data collection. In this new context, researchers need to obtain and reuse social media data with ethical standards that prevent harm to individuals and communities.  ","version":"Next","tagName":"h2"},{"title":"A principle-based framework​","type":1,"pageTitle":"Ethical Considerations","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations#a-principle-based-framework","content":" In response to the dynamic nature of social media, this chapter introduces you to a principles-based approach to Social Media Monitoring. A set of principles that, when applied, provide a framework and a living process that allow you to mitigate risks by responding promptly to ethical challenges as they emerge. Initially, the framework aims to understand risks and create a robust foundation for addressing the multilayered complexities of handling social media data ethically in various contexts. This includes transparency about data collection practices, privacy concerns, bias and discrimination, and cultural diversity. Understanding that ethical challenges are complex is key to acknowledging that research design will naturally pinpoint ethical risks if some principles are applied to the process. Next, the chapter includes a Key Risk Indicator (KRI) to identify potential harms at different stages of social media data projects that should be regularly monitored to trigger proactive responses. Conducting a risk control self-assessment and including corrective actions and adjustments to ethical frameworks and practices is essential to improve the safety and quality of social media data research. Finally, the framework provides an easy-to-understand principles table that can be applied to the real-time design, deployment and evaluation of projects involving social media monitoring. This framework can help researchers and activists working on social media monitoring clarify their implicit or explicit decisions when collecting data and help define an ethical approach to their work.  ","version":"Next","tagName":"h2"},{"title":"Ex-ante considerations of handling social media data ethically​","type":1,"pageTitle":"Ethical Considerations","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations#ex-ante-considerations-of-handling-social-media-data-ethically","content":" When monitoring social media, many researchers could potentially collect data in real-time first and only later consider the ethical implications or potential risks once the dataset is completed. An easy justification for such behaviour could be that the information is “out in the public” anyway and that its massive collection is valid, justified, and harmless. The first step towards more ethical data collection is challenging this implicit “scrape all the data” consideration many data collectors embrace.  The following list is not exhaustive but sheds light on interconnected and multilayered risks that monitoring social media can present across different contexts, and where research is not clearly delineated. As a researcher you should think through these implications before you start your monitoring efforts:  Human Rights considerations  Right to Oblivion or Right to be Forgotten: The right to oblivion, or the right to be forgotten, is a concept rooted in Article 12 of the Universal Declaration of Human Rights, also recognised by courts in the EU, Argentina and The Philippines. It asserts that no one shall be subjected to arbitrary interference with their privacy, so individuals should be able to request the removal of personal data that is no longer necessary. Social media monitoring can challenge this right, as data collected may persist indefinitely, potentially haunting individuals long after the information loses relevance. Vulnerability: Vulnerable populations, including minors, marginalised communities, individuals with limited digital literacy and people suffering from abuse, can be disproportionately affected by social media monitoring. They may lack the resources and knowledge to challenge the exploitation of their data and make informed choices, including the right to challenge the very basis of monitoring. It is important to note that vulnerability highly depends on local context. A person can become vulnerable because of local laws or due to culture, and a person may be vulnerable in one country but not another. Local context always needs to be considered. Protection of Personal Data: Failing to comply with data protection regulations, such as the EU’s General Data Protection Regulation (GDPR), regarding the processing of personal data can lead to data breaches and misuse of individuals' personal information. Non-Discrimination: Discriminatory practices in social media data collection or algorithmic bias can lead to unequal approaches based on race, gender, or religion and reinforce existing inequalities in certain groups.  Social and collective harm  Data Extractivism: The mass extraction of social media data involves collecting data from individuals or organisations without their informed or free and informed consent and can be linked to resource extraction. This extractive approach can lead to exploitation, as individuals become commodities in the data-driven economy, raising concerns around fairness and the concentration of power. The expression “data extractivism” establishes an analogy between information management and the mining industry, defining data as a raw material that can be extracted, commercialised, refined, processed, and transformed into other commodities with added value. Power Dynamics: The concentration of data in the hands of a few can shift power dynamics in society and lead to creating data monopolies. This centralised control raises concerns about surveillance states and the potential for abuse of power, as well as the erosion of democratic principles as data owners can significantly influence markets, decision-making, and even public discourse. Economic Harm: The vast amount of data collected from social media users has great economic value in the digital economy, leading to concerns about economic harm to underprivileged groups. Economic harm from unethical or malicious social media monitoring practices can raise questions about fairness and equity.  ","version":"Next","tagName":"h2"},{"title":"First Proposal: Develop Social Media Monitoring Key Risk Indicators (KRI)​","type":1,"pageTitle":"Ethical Considerations","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations#first-proposal-develop-social-media-monitoring-key-risk-indicators-kri","content":" Is the data I am collecting ethical? There is no simple answer to this question, but you can adopt Key Risk Indicators (KRI) that present an easy way to help you identify potential harms. KRI are specific metrics that serve as early warning signs for social media monitoring that can be used when researchers design projects to track ethical risks associated with their projects.  Risk\tIndicator\tImpactOpaque Data Usage\tFull disclosure of documentation regarding data collection practices and purposes.\tA low transparency rate may indicate an ethical risk related to the lack of openness about data usage. Non-Compliance\tPercentage of data where informed consent was obtained for collection.\tA low compliance rate may indicate a potential ethical risk related to data collection practices. Weak Data Accuracy\tQualitative analysis of data in terms of misinformation or manipulated content.\tWeak data accuracy can indicate an ethical risk, especially when used in decision-making processes. Violation of Data Rights\tThe number of disputes related to data ownership.\tAn increased ownership dispute rate can indicate ethical concerns about data rights. Biased data\tNumber of occurrences of bias detected in data collection and analysis.\tRepeated detection of bias can indicate ethical concerns regarding fairness and discrimination.  ","version":"Next","tagName":"h2"},{"title":"Second Proposal: Adopt a Principles-Based Approach for Social Media Data Handling​","type":1,"pageTitle":"Ethical Considerations","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations#second-proposal-adopt-a-principles-based-approach-for-social-media-data-handling","content":" In a fast-changing scenario like the social media environment, new circumstances that are not contemplated can arise at any time, making it difficult to stay up to date when using a strict framework. Thus, a rules-based approach can quickly become outdated. A principles-based approach, however, can help you, as researchers, reduce the complexity of compliance. It also promotes collaboration between different stakeholders, which speeds up the process and empowers people to take ownership.  This set of key principles and ethical standards are aimed at guiding organisations and researchers in monitoring social media platform data to ensure its responsible and ethical use, respecting user privacy, and promoting transparency.  Principle\tIssue\tProcessTransparency\tAm I being transparent about the purpose of social media monitoring and the data types I am collecting?\tInform users about how their data may be used and who has access to it. Apply FAIR Principles to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. Compliance\tAm I respecting the relevant laws and regulations, including data privacy and intellectual property laws?\tEnsure that your monitoring practices comply with contextualised laws and allow users to opt out of being monitored. Data Accuracy\tAm I ensuring that the information gathered from social media is credible and verifiable?\tAvoid spreading misinformation by establishing an information-checking process before considering the data in the research. Data integrity and availability should also be addressed. Explicit Consent\tAm I seeking explicit permissions from users when collecting data that might be considered sensitive?\tDetermine how the data will be used, and respect users' decisions regarding data usage and sharing. Data Minimization and Deletion\tAm I ensuring that only personal data that is directly related to the research is collected?\tLimit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose. Data that is not needed anymore should be deleted. Bias Detection\tAm I ensuring that social media monitoring and analysis do not lead to discrimination based on race, gender, religion or political angle?\tFoster diversity and inclusion within your monitoring team. If using algorithms for data analysis, make them transparent and auditable. Community Feedback\tAm I being open to feedback from the community regarding potential monitoring activities?\tActively seek input by providing accessible channels for users to voice their concerns, suggestions, and questions. Incorporate feedback when necessary. Accountability\tAm I regularly reviewing and auditing the monitoring practices to ensure compliance with ethical principles?\tEstablish accountability mechanisms by assigning responsibility for social media monitoring within the organisation. Openness\tAm I ensuring people can access and reuse all the tools and materials from the social media monitoring project?\tComply with the Open Definition principles and provide an open licence for other people to access and reuse all the resources. Beware of Vulnerabilities\tAm I recognising the potential harm social media monitoring can cause vulnerable individuals?\tOnly collect, share, or analyse data related to vulnerable individuals' experiences when it serves a legitimate and beneficial purpose. Pseudonymize or anonymize the data as much as possible to prevent harm Periodic Review\tAm I regularly reviewing monitoring practices to align with user behaviour and platform policy evolution over time?\tRegularly adapt monitoring practices to align with ethical standards that relate to the current reality.  ","version":"Next","tagName":"h2"},{"title":"References and further resources​","type":1,"pageTitle":"Ethical Considerations","url":"/data-knowledge-hub/docs/get-started/01_02_ethical-considerations#references-and-further-resources","content":" Mehtab Khan and Alex Hanna. The Subjects and Stages of AI Dataset Development: A Framework for Dataset Accountability. September 13, 2022.  Adriana Alvarado Garcia, Marisol Wong-Villacres, Milagros Miceli, Benjamín Hernández, Christopher A Le Dantec. Mobilizing Social Media Data: Reflections of a Researcher Mediating between Data and Organization. CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. April 2023 Article No.: 866 Pages 1–19 https://doi.org/10.1145/3544548.3580916 .  Connie Moon Sehat and Tarunima Prabhakar, with Aleksei Kaminski. Ethical Approaches to Closed Messaging Research. Considerations in Democratic Contexts. March 15, 2021.  Sara Mannheimer and Elizabeth A. Hull. Sharing selves: Developing an ethical framework for curating social media data. 12th International Digital Curation Conference (IDCC), Edinburgh, Scotland, 20-23 February 2017.  Dr. Leanne Townsend and Prof. Claire Wallace. Social Media Research: A Guide to Ethics. This work was supported by the Economic and Social Research Council [grant number ES/M001628/1] and was carried out at The University of Aberdeen, 2016.  Horbach SPJM, Tijdink JK, Bouter L. 2022 Research funders should be more transparent: a plea for open applications. R. Soc.Open Sci.9: 220750. https://doi.org/10.1098/rsos.22075  Celis Bueno y Schultz (2021) Data Extractivism. In: Celis Bueno y Schultz, Imaginación maquínica, http://imaginacionmaquinica.cl/data-extractivism ","version":"Next","tagName":"h2"},{"title":"Legal context of monitoring digital discourse on social media platforms","type":0,"sectionRef":"#","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations","content":"","keywords":"","version":"Next"},{"title":"1. Useful legal concepts when analysing social media platforms​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#1-useful-legal-concepts-when-analysing-social-media-platforms","content":" Social media platforms are pervasive in our lives and deeply entangled with the exercise of some of our rights and many of the aspects of our public life. Yet, most of these platforms were not created with this in mind. On the one hand, we can use them to exercise our freedom of expression, our right of association or our freedom to conduct a business. On the other, we must not forget that by doing so we are implicitly trusting that their legitimate owners will be collaborative and supportive to everyone’s self-determination even though, legally speaking, they are not required to do so.  Problems connected to this circumstance can be found in contested practices such as “deplatforming” (1) users, which consists of removing and banning an individual or a group from a mass communication medium such as a social networking or blogging website. The case is similar for so called “shadow banning” (2), which is the practice of blocking or partially blocking a user or a user's content from some areas of an online community in such a way that the ban is not readily apparent to the user.  The decision on whether and how to enforce a ban from a social media platform can be taken unilaterally and with a lack of transparency because these spaces constitute private property, and it is up to the platform providers to decide who can access them and who cannot.  This very same logic applies when we look at the analysis of platforms for research purposes.  While law makers and regulators are still in the process of working towards a common set of rules for online platforms that would apply internationally, it is essential for you to be aware that different countries have their own approaches to regulating these platforms. This means that the way online platforms are managed can vary from one country to another. However, this variation should not overwhelm you. There are some fundamental principles that you can keep in mind when considering how these platforms are controlled.  It's important to recognize that the internet does not have strict, universal rules that every country follows. Instead, each nation has the authority to make its own regulations regarding online platforms. These rules can cover a wide range of topics, including how data is handled, what content is allowed and how businesses compete.  As a researcher of online platforms, it's a good idea to stay informed about the specific rules that apply in the country or countries where you are operating. This knowledge will help you navigate the digital landscape more effectively and responsibly. Additionally, understanding that while rules may differ, some core principles remain constant can serve as a compass for your online exploration.  What is the legal concept of a digital platform?  One of the key principles that underpin all legislative efforts in this field is the legal notion of “digital asset”, which is anything that exists only in digital form and comes with a distinct usage right or distinct permission for use. Digital assets encompass a wide range of items, such as software, photographs, logos, illustrations, animations, audiovisual content, presentations, spreadsheets, digital artwork, word documents, email messages, websites, and numerous other digital formats, along with their associated metadata.  From a legal standpoint, these kinds of assets represent an extension in the digital world of one's physical assets and virtual space being part of an individual's personal sphere that, as such, deserves legal protection.  Following this logic, online platforms are considered digital assets owned by private subjects to which a very ancient legal concept applies that since Latin time was known as ius excludendi alios, literally “the right to exclude others” (3). This concept refers to the fundamental property right that allows a property owner to exclude others from using or entering their property and to create a set of rules to be followed while staying inside of it. In essence, it is the right to control access to and use of one's property and is a key element of property ownership in many legal systems. It means that the property owner has the authority to determine who and under which conditions can access their property.  If you want to monitor and analyse social media platforms, notably without their consent, you need to be prepared that this could represent a violation of this “right to exclude others” and consequentially translate into civil or even criminal liability, if not both. Platform owners could decide to carry legal actions against people performing social media analysis or, at the very least, permanently ban them from the platform.  For the scope of this chapter, we will not delve into the ramifications of criminal liability, as matters of criminal law significantly vary from nation to nation within their specific legal systems. It is just worth specifying that often this form of legal responsibly involves the actual violation of security measures put in place on a given platform and, in most cases, the actual intentionality to temper with or damage the digital infrastructure (even if this is still highly debated among scholars).  What kind of civil liability can derive from platform analysis?  Another key principle that must be considered relates to the risk of civil liability, which is usually distinguished in contractual liability or tort liability. In brief, contractual liability arises from the breach of an agreement stipulated between parties, while tort liability is the responsibility to compensate for harm or injury caused to another person or their property through wrongful conduct outside of a contract.  In the field of social media platform analysis, you are most likely to be liable at a civil level if you breach the terms of services and/or user agreements, infringe on copyright, intellectual property, or trade secrets. In addition, you be held liable if you violate data protection law or the privacy of other users.  Examples of tort liability can cover a wide range of civil wrongs, such as the disruption of the service provided by a platform or the violation of its trade secrets. This can result in compensation or remedies for the damaged party.  The interesting part, however, is that there is much to learn about the ethics and the behaviour of big platforms also from the way they position themselves inside this vast and often convoluted legal context. In a certain sense, analysing their legal practices can be already revealing of the way they run their business.  The Digital Services Act allows so-called “vetted researchers” (4) to file for data access to conduct independent audits, which significantly increases the possibilities for public scrutiny of the digital activities on social media platforms. This research access is promising and important though the specifics of this procedure are yet to be decided. It will be interesting to see how platforms comply with and implement these new rules.  ","version":"Next","tagName":"h2"},{"title":"1.1 Terms of Services and User Agreements: What do they mean for the Analysis of Social Media Platforms?​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#11-terms-of-services-and-user-agreements-what-do-they-mean-for-the-analysis-of-social-media-platforms","content":" Every user who navigates the internet, whether using social media platforms, e-commerce marketplaces, streaming services, and others, will at some point be confronted with the famous “wall of text” of the online world: the Terms of Services (ToS).  These ToS govern our relationship with platforms, delineate our rights and impose restrictions that bear profound legal implications. They represent a significant portion of the conditions under which the owners of a certain platform are willing to allow other people to interact with their digital assets and, most importantly, constitute part of an agreement (a contract, in many ways) between user and platform. The work of demystifying these documents can help lawyers, scholars, and researchers in having a better understanding of the actions they are allowed to perform and sometimes also offer a reference point between what is supposed to happen on a platform and what is possible to observe.  At first glance, Terms of Services and user agreements may appear verbose, intricate, and dense. However, beyond the intricate legal jargon lies an interplay of restrictions and allowances that determine how platforms can be analysed, studied, and critiqued. In an age where digital platforms influence economies, democracies, and personal lives, understanding these limitations represents a precious strategic advantage.  One primary concern revolves around data access and the possibility of monitoring and understanding how the platform works. Platforms, through their ToS, often restrict how users can collect data, analyse it, or share it. For researchers aiming to study platform algorithms, behaviours or biases, these restrictions can severely impede transparency and understanding of the concrete underlying mechanisms that they are trying to observe. There is a delicate balance to strike between data ownership and platform integrity and the actual ability to study these platforms and hold them accountable in case of potential malpractices.  Can you use automated tools to analyse social media platforms?  Although it is always necessary to make important distinctions case by case, one general tendency that often characterises the prescriptions of terms of service imposed by social media platforms, is the prohibition for users to utilise automated tools when interacting with the digital infrastructure. Techniques like “web scraping” (5) or the usage of a “bot net” (6) are almost always forbidden by the ToS of the major social media platforms (More on webscraping in the chapter here). Researchers must therefore carefully weigh whether they resort to other methodologies to carry on their work.  One alternative may be the use of Application Programming Interface (API) (More on APIs in the chapters on Twitter and on TikTok) that many websites and online services provide directly to allow developers to access and retrieve data in a structured and standardised way. APIs are a more reliable and less ethically challenging method of gathering data compared to web scraping because they are designed for data exchange. Researchers can use programming languages like Python to interact with APIs and retrieve data.  Another solution can be to rely on public datasets (More on public datasets in the chapter here), which many organisations and institutions make publicly available for research and analyses so that these datasets can be used for various purposes for free. When possible, researchers may also purchase datasets from data providers and vendors, which provide datasets that are often clean, structured, and ready for analysis. Alternatively, there is always the solution of manual data entry; while it is the most time-consuming method, manual data entry involves copying and pasting information from web pages into a spreadsheet or database and can be suitable for small-scale data extraction tasks.  However, it is worth specifying that the use of automation in platform monitoring is not per se legally forbidden, notably in cases in which it is used to speed up the process of gathering publicly available data from a website and does not interfere with the regular functioning of a platform’s servers.  The legitimacy of techniques like scraping is contested. The research community may point to norms such as the Digital Services Act as a legal foundation, yet the legal argument is challenging and there are always ethical considerations that must be weighed carefully.  This goes to say that the use of automation aimed to analyse social media platforms can surely give ground, in those cases when the ToS forbids such practice, to the issuance of a ban from said platform. However, this will not necessarily translate into the insurgency of civil liability. After all, preserving the agreement between the user and the owner of a platform is something significantly different than interpreting and enforcing the law in a way that solely the jurisdictional power can do.  What to pay attention to when reading ToS?  It does not come as a surprise then, that these agreements often contain clauses pertaining to dispute resolution, often pushing disagreements into arbitration, and preventing class-action lawsuits. Such stipulations can limit the tools available for users and researchers to challenge or question platform actions, thereby creating potential power imbalances between individual users and tech-giants.  Yet, while these agreements impose limitations, they also provide insights. A meticulous examination of a platform's ToS can reveal much about its priorities, ethics, and business model. Is the platform user-centric or does it prioritise its economic objectives? Do its legal stipulations push for transparency or opacity? The answers to these questions may appear in the lines of user agreements and are essential for any comprehensive platform analysis.  The road ahead for lawyers and researchers is both challenging and fascinating. As we explore these agreements, we must develop methodologies to analyse platforms within their legal bounds while advocating for transparency, accountability, and fairness. Strategies may range from seeking amendments in these agreements to leveraging technology for compliant data collection and analysis.  ","version":"Next","tagName":"h3"},{"title":"1.2 Reflecting on Copyright, Intellectual Property and Trade Secret: Implications for Platform Analysis​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#12-reflecting-on-copyright-intellectual-property-and-trade-secret-implications-for-platform-analysis","content":" In the digital age, where ownership over immaterial goods grows in its importance and tireless innovation is driving progress, the rules about copyright, intellectual property (IP), and trade secrets become indicators of great significance. These are the rules upon which digital platforms are built and for anyone studying these platforms, it is crucial to understand how these legal instruments work together.  Why does copyright law matter in social media analysis?  The field of copyright law has undergone a profound transformation in the past decades. In this era of rapid information exchange, content is disseminated, reimagined, and redistributed at an unprecedented pace, presenting a myriad of challenges for platforms when it comes to ascertaining content ownership and navigating usage rights. This evolving landscape has given rise to intriguing questions, such as how one can attribute originality in a meme culture (7) and how to determine the bounds of fair use in an environment teeming with remixes and mashups.  A clear example of this comes from the many legal battles started against the practice of training generative artificial intelligence models using copyrighted material of renowned writers. As of today, for instance, there are three different lawsuits for copyright infringement lodged against the renowned company OpenAI (8).  For researchers, the intricate layers of copyright law wield a profound influence on how content on digital platforms can be utilised, analysed, and critiqued. The shifting landscape demands a nuanced understanding of copyright principles, such as the doctrine of fair use, which varies from jurisdiction to jurisdiction. This requires a careful navigation of the legal landscape to avoid the potential pitfalls of copyright infringement. Researchers must be vigilant in respecting the rights of content creators while pursuing their academic or investigative endeavours.  What can we learn about a platform from its Intellectual Property portfolio?  In the broad spectrum of legal rights pertaining intellectual property, it is possible to observe yet another dynamic aspect of the strategies implemented by big platforms.  IP rights provide a safeguard to platform's innovations, fortifying their position and guaranteeing the inviolability of their distinctive value propositions. Through this defensive role, platforms can enjoy a sense of security, knowing that their creative outputs and innovative solutions are shielded from unauthorised duplication, thereby preserving their exclusivity in the market and sustaining customer loyalty.  At the same time, intellectual property can also provide platforms with the means to assert dominance within market landscapes. With this offensive capability, platforms can strategically employ their IP rights to assert their presence and challenge competitors. By aggressively leveraging their patents, trademarks and copyrights, platforms can establish a formidable presence in their respective domains, effectively carving out their niches and dissuading rivals from encroaching on their territory. Moreover, IP rights grant platforms the authority to influence industry standards, regulatory frameworks and even the direction of the market, thereby solidifying their position as industry leaders.  For analysts dedicated to deconstructing platforms and their strategies, a comprehensive understanding of the platform's intellectual property portfolio proves indispensable. Such insight serves as a valuable observation point to the platform's inner workings, strategic priorities, and competitive positioning. By scrutinising the breadth and depth of a platform's IP holdings, researchers can discern the platform's focal points, for instance whether it is heavily invested in technological innovation or brand recognition. Additionally, the assessment of a platform's intellectual property portfolio unveils potential vulnerabilities, helping analysts identify areas where the platform might be exposed to competitive threats or imitation.  How relevant are platform's trade secrets?  Things are significantly different with trade secrets. Unlike patents, which require public disclosure and grant exclusive rights for a limited period, trade secrets have the role to protect a platform's core functionalities and innovations.  In the current technology-driven market, trade secrets serve as safeguard for the proprietary knowledge that allows big platforms to maintain their competitive advantage. These legal instruments protect a diverse set of elements that are vital for social media platforms, including their algorithms responsible for making their user experience as engaging and irresistible as possible; their proprietary data processing techniques that underpin automated decision-making and their distinctive business strategies.  Delving into the inner workings of a platform without violating its trade secrets is indeed a challenging mission. It requires a delicate balance between technical skills and legal prudence. The technical finesse required involves a deep understanding of complex algorithms and data processing methods, enabling analysts to infer a platform's functionality without direct access to its proprietary code. This often entails reverse engineering, complex data analysis and other advanced techniques that enable a comprehensive assessment of the platform's capabilities. Simultaneously, legal knowledge is paramount in ensuring that this exploration remains within the boundaries of the law. As trade secrets are legally protected, analysts must be equipped with a keen awareness of the limits imposed by trade secret laws and the need for ethical and lawful analysis.  Beyond the individual intricacies of copyright, IP and trade secrets, their convergence brings forth unique challenges and considerations. Platforms often operate in ecosystems where these legal constructs intertwine, creating an overlap of rights and restrictions that researchers must adequately navigate. Additionally, with platforms often operating across borders, the international dimensions of these laws add yet another layer of complexity.  ","version":"Next","tagName":"h3"},{"title":"1.3 Privacy and Data Protection Law Compliance in Analysing Social Media Platforms​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#13-privacy-and-data-protection-law-compliance-in-analysing-social-media-platforms","content":" At the very core of what represents the economic strength of large online platforms there is beyond any doubt the incredible quantity of personal data that they are capable to collect and process daily. This unprecedented power has been able to transform consolidated market dynamics on a global scale.  One pressing challenge faced by researchers and analysts, who delve into the task of understanding and examining these platforms is to be able to investigate this data-rich landscape while ensuring strict adherence to privacy and data protection laws.  Is data-protection law a tool or a limit?  A foundational understanding begins with recognising the importance of individual privacy rights in the digital context. In essence, every user's interaction with a social media platform is, in many jurisdictions, considered a piece of personal data and a fragment of one's digital identity. As such, the law meticulously guards against any unauthorised or non-compliant access, use, or dissemination of this data.  The General Data Protection Regulation (GDPR) (9) of the European Union and the California Consumer Privacy Act (CCPA) (10) in the United States stand as prominent examples of legal regimes that set rigorous standards for data protection. While the specifics may vary, the core principles remain consistent: personal data must be processed lawfully, transparently and for a specific purpose; it must be minimal, accurate and stored only as long as necessary; individuals have the right to access, correct or delete their data.  For platform analysts, these laws carve out both a roadmap and a minefield. A roadmap in the sense of providing structured guidelines on lawful data processing. A minefield because inadvertent non-compliance can lead to severe legal consequences, both in terms of penalties and reputational harm.  How can we use people's personal data in a safe and ethical way?  A particular issue pertains the topic of “informed consent”. When users sign up for social media platforms, they often provide consent for data collection regulated by terms of service or privacy policies. However, this consent may not extend to third-party analyses or research and analysts must, therefore, be careful in ensuring that their data access and processing methods either fall within the coverage of existing consents or have separate, explicit permissions.  In this regard, a very inspiring practice is represented by the “data donation” (11), which is the voluntary act of individuals or organisations contributing their data for research, social good or humanitarian purposes. This concept involves people or entities choosing to share their data in various ways to support different objectives such as the research on climate, healthcare, or social media platforms.  Additionally, the rise of 'anonymised' or 'de-identified' data offers both opportunities and challenges. While such data sets, stripped of personal identifiers, can potentially be used without infringing on individual privacy rights, questions arise about the true effectiveness of anonymisation techniques, especially given the sophistication of modern re-identification methods. Moreover, it is crucial for the anonymisation procedure to take place directly on the device of the interested data subject, as it would otherwise represent very likely an actual processing of personal data.  ","version":"Next","tagName":"h3"},{"title":"2. Considerations on lawful and crucial support from IT experts​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#2-considerations-on-lawful-and-crucial-support-from-it-experts","content":" Once we have delineated the main legal profiles that shape and constrain the landscape of social media analysis, we can transition from the juridical outlines to the technical aspects connected with the previous considerations. In the digital era, law and technology influence each other at a growing pace and the same is true for legal experts and information technology specialists.  IT experts, in this context, emerge as both decipherers and architects that can unravel these intricate patterns, offering insights that surpass simple interactions with a platform. Their contributions can, for instance, manifest itself in the creation of customised tools specifically engineered to enhance the precision and efficiency of social media monitoring. These instruments often may consist in specific customised analysis tools capable of performing small tasks tailored for the needs of each project.  In this section of the chapter, we will discuss some key aspects of how to document and describe the analysis operations performed on a digital platform through a technical report. Moreover, we will ponder on how a strong technical expertise in the field of IT can be leveraged to even make the platforms want to be analysed it the first place.  ","version":"Next","tagName":"h2"},{"title":"2.1 Best Practices in Evidence Gathering and Drafting of Technical Reports​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#21-best-practices-in-evidence-gathering-and-drafting-of-technical-reports","content":" To sustain any form of argumentation, whether for a legal case, a journalistic inquiry or academic research, it is always of the utmost importance to be able to count on solid and consistent evidence. The same applies when analysing social media platforms, where you may need to master the art of digital forensics to gather evidence in legal and ethical manner.  Technical reports are integral components of the narratives necessary to be built around a meaningful inquiry and they must be meticulously crafted, ensuring clarity, credibility, and compliance with both the relevant legal framework and the best practices of the field.  There is a remarkable diversity of digital evidence in today's digital landscape, depending on the concrete scope of one’s research. From emails to server logs, social media interactions to encrypted messages, the digital footprint left by individuals and entities is vast and varied. Navigating this expansive domain necessitates rigorous protocols to ensure the authenticity, integrity, and admissibility of evidence.  The first and most important concept for a solid acquisition of digital evidence is the so called “chain of custody” (12), which is the order and way evidence has been handled in an uninterrupted process that can guarantee that is has not been altered. Establishing a clear chain of custody from the moment of data acquisition to its presentation is paramount and every transfer, storage or access point must be meticulously logged, ensuring that the evidence remains untainted and its origins traceable.  Preservation of the original state is key to insure the authenticity of the evidence. Tools like forensic disk imaging can capture exact replicas of digital media, preserving metadata and ensuring that the original source remains unaltered. For the selection of the best evidence gathering tools is convenient to pay special attention to the level of accuracy that they are capable of, as this will have a great impact on the credibility of our claims and their ability to stand against counterarguments.  The same level of care shall be given to the actual draft of the technical report that will describe the evidence and how the acquisition procedure looked like with a consistent level of clarity, comprehensibility, and precision. A technical report must be easily navigable, with a clear structure and layout in which there must be the description of (at least) the methodology used, findings obtained and conclusions derivable from them. The language used should ensure that even the readers unfamiliar with technical jargon can be able to follow the narrative.  Given that technical reports often find their way into legal settings where technical expertise may be limited, including a plain language summary in the introduction can bridge the gap and offer a concise and comprehensible overview of the report's key findings.  Detailing every step of the methodology used for the evidence gathering process, the characteristics of the tools used and the rationale behind each decision help in making the report reliable and allows for its reproducibility. Visual aids like graphs, charts and tables can simplify complex data sets, providing clear visual representations that enhance understanding. However, each visual aid must be accompanied by comprehensive captions and sourced appropriately to avoid the risks of misinterpretation.  Before finalising a technical report, it is advisable to subject it to peer review by external experts that can validate the methodology, highlight potential omissions, and ensure that the report stands up to scrutiny.  ","version":"Next","tagName":"h3"},{"title":"2.2 Ethical Hacking and Security Research Exemptions: Balancing Vulnerability Disclosure and Platform Integrity​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#22-ethical-hacking-and-security-research-exemptions-balancing-vulnerability-disclosure-and-platform-integrity","content":" There is one more way to go very deep into the analysis of large online platforms, or any digital infrastructure, and benefit from a sort of exemption that facilitate the research and reduces the risks of repercussions. It is called “ethical hacking” (13) and it consists in investigating a platform to identify vulnerabilities, bugs, and other defects without a malicious intent, but to improve the security and performance of a certain place in the cyberspace.  Guiding principle of an ethical hacker, also called “white-hat hacker” is to uncover weaknesses of a platform so they can be rectified. This is the core difference with the so called “black-hat hackers”, who seek vulnerabilities to exploit them or for other malevolent reasons.  Numerous jurisdictions have begun recognising the invaluable contributions of ethical hackers. Laws have evolved, creating exemptions for security research, ensuring that well-intentioned hackers do not find themselves inadvertently on the wrong side of the law. However, these exemptions come with conditions, ensuring the research does not compromise user data, platform stability or infringe on intellectual property.  In a collaborative stride, many platforms now host Vulnerability Disclosure Programs, inviting ethical hackers to identify and report vulnerabilities and these programs provide a structured framework, often with guidelines on safe testing, responsible disclosure, and rewards for researchers. These guidelines are very important because ethical hackers, for example when probing social media platforms, risk accessing user data or exceeding the scope of the research they were supposed to conduct. Striking a balance between research comprehensiveness and user privacy is critical and ethical hackers must employ techniques to test vulnerabilities without compromising or accessing real user data.  One of the pivotal dilemmas in ethical hacking is the timing of vulnerability disclosure. While immediate disclosure seems the most transparent approach, it might leave platforms exposed until a fix is implemented. Yet, delayed disclosure might provide ample remediation time but could risk undisclosed vulnerabilities being discovered and exploited by malicious actors.  It is worth noticing that different national jurisdictions can often have differing stances on ethical hacking and security research and, for this reason, understanding and navigating these legal intricacies is imperative to ensure this form of research remains compliant even when conducted across borders. As technology advances, the realm of ethical hacking will inevitably evolve and understanding the new trends can prepare platforms and researchers for forthcoming cybersecurity landscapes.  ","version":"Next","tagName":"h3"},{"title":"3. Navigating the Legal Terrain of Social Media Monitoring​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#3-navigating-the-legal-terrain-of-social-media-monitoring","content":" As we conclude this chapter, it is important to emphasise few steps that you may follow to ensure that your efforts in monitoring online discourse remain legally sound.  Before embarking on any social media monitoring endeavour, it is essential to acquaint yourself with the relevant laws and regulations governing online activities. Each jurisdiction may have its own set of rules governing data privacy, intellectual property, defamation and more. Additionally, meticulously review the Terms of Service (ToS) of the social media platforms you intend to monitor. These agreements often contain provisions addressing data usage, scraping, and content sharing, which can significantly impact your monitoring activities. Seek the assistance of a lawyer or a legal expert that you trust, as you start detailing the framework of your research.  Respect for user privacy is a cornerstone of responsible social media monitoring. When collecting and analysing data that involves individuals, seek informed consent whenever possible. This not only aligns with ethical research practices but also mitigates potential legal concerns. Furthermore, when sharing or presenting your findings, take diligent steps to anonymize data and protect the identities of users, unless explicit consent has been granted for their inclusion.  Important note It is always advisable to seek guidance from a lawyer or a legal expert knowledgeable of IT law and the specific characteristics of the legal system applicable in the jurisdiction in which you will operate. This can be paired with the invaluable contribution of technology experts.  As researchers exploring the expansive realm of social media monitoring, you have the fundamental role to uncover insights, influence discussions and contribute to the collective understanding of online discourse. This is a power that comes with great responsibility, and your commitment to adhering to legal and ethical principles is paramount. By being aware of the current context described in this chapter, we can navigate the legal terrain of social media monitoring with confidence ensuring that our research remains both legally sound and ethically responsible.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Legal context of monitoring digital discourse on social media platforms","url":"/data-knowledge-hub/docs/get-started/01_03_legal-considerations#references","content":" (1) A. Mekacher1, M. Falkenberg and A. Baronchelli, The Systemic Impact of Deplatforming on Social Media, 2023  (2) https://www.nytimes.com/interactive/2023/01/13/business/what-is-shadow-banning.html  (3) https://www.oxfordreference.com/display/10.1093/acref/9780195369380.001.0001/acref-9780195369380-e-1127  (4) https://digitalservicesact.cc/dsa/art31.html  (5) V. Singrodia, A. Mitra, S. Paul, A Review on Web Scrapping and its Applications, 2019  (6) N. Kaur, M. Singh, A Review on Web Scrapping and its Applications, 2016  (7) S. Blackmore, The Meme Machine, published by Oxford University, 2000  (8) https://www.reuters.com/technology/more-writers-sue-openai-copyright-infringement-over-ai-training-2023-09-11/  (9) https://eur-lex.europa.eu/eli/reg/2016/679/oj  (10) https://oag.ca.gov/privacy/ccpa  (11) J. Ohme, T. Araujo, Digital data donations: A quest for best practices, 2022  (12) M.N.O. Sadiku, A.E. Shadare,S.M. Musa, Digital Chain of Custody, 2017  (13) Hafele, Three Different Shades of Ethical Hacking: Black, White and Gray, 2021 ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}